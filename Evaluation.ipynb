{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc37f7ef",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this notebook, the IGANN-IT model is applied to 23 structured data sets and compared with interpretable and non-interpretable baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7662a6bd",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "32bac5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score, log_loss, average_precision_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Lasso, Ridge, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from interpret.glassbox import ExplainableBoostingRegressor, ExplainableBoostingClassifier\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "import igann\n",
    "importlib.reload(igann)\n",
    "from igann import IGANN\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4e9c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/maximilianveitl/data/shared\"\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d09e8eb",
   "metadata": {},
   "source": [
    "# 3. Overview Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b738bd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>file_name</th>\n",
       "      <th>task_type</th>\n",
       "      <th>n_num_features</th>\n",
       "      <th>n_cat_features</th>\n",
       "      <th>target</th>\n",
       "      <th>separator</th>\n",
       "      <th>folder_name</th>\n",
       "      <th>num_features</th>\n",
       "      <th>cat_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stroke</td>\n",
       "      <td>healthcare-dataset-stroke-data.csv</td>\n",
       "      <td>binary</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>stroke</td>\n",
       "      <td>,</td>\n",
       "      <td>shared</td>\n",
       "      <td>[age, avg_glucose_level, bmi]</td>\n",
       "      <td>[gender, hypertension, heart_disease, ever_mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>churn</td>\n",
       "      <td>WA_Fn-UseC_-Telco-Customer-Churn.csv</td>\n",
       "      <td>binary</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>Churn</td>\n",
       "      <td>,</td>\n",
       "      <td>shared</td>\n",
       "      <td>[tenure, MonthlyCharges, TotalCharges]</td>\n",
       "      <td>[gender, SeniorCitizen, Partner, Dependents, P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fico</td>\n",
       "      <td>fico_heloc_dataset_v1.csv</td>\n",
       "      <td>binary</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>RiskPerformance</td>\n",
       "      <td>,</td>\n",
       "      <td>shared</td>\n",
       "      <td>[ExternalRiskEstimate, MSinceOldestTradeOpen, ...</td>\n",
       "      <td>[MaxDelq2PublicRecLast12M, MaxDelqEver]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bank</td>\n",
       "      <td>bank-full.csv</td>\n",
       "      <td>binary</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>y</td>\n",
       "      <td>;</td>\n",
       "      <td>shared</td>\n",
       "      <td>[age, balance, day, campaign, pdays, previous]</td>\n",
       "      <td>[job, marital, education, default, housing, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adult</td>\n",
       "      <td>adult_census_income.csv</td>\n",
       "      <td>binary</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>income</td>\n",
       "      <td>,</td>\n",
       "      <td>shared</td>\n",
       "      <td>[age, fnlwgt, education.num, capital.gain, cap...</td>\n",
       "      <td>[workclass, education, marital.status, occupat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>airline</td>\n",
       "      <td>airline_train.csv</td>\n",
       "      <td>binary</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>satisfaction</td>\n",
       "      <td>,</td>\n",
       "      <td>shared</td>\n",
       "      <td>[Age, Flight Distance, Inflight wifi service, ...</td>\n",
       "      <td>[Gender, Customer Type, Type of Travel, Class]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>college</td>\n",
       "      <td>college_data.csv</td>\n",
       "      <td>binary</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>will_go_to_college</td>\n",
       "      <td>,</td>\n",
       "      <td>shared</td>\n",
       "      <td>[parent_age, parent_salary, house_area, averag...</td>\n",
       "      <td>[type_school, school_accreditation, gender, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weather</td>\n",
       "      <td>weatherAUS.csv</td>\n",
       "      <td>binary</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>RainTomorrow</td>\n",
       "      <td>,</td>\n",
       "      <td>shared</td>\n",
       "      <td>[MinTemp, MaxTemp, Rainfall, Evaporation, Suns...</td>\n",
       "      <td>[Location, WindGustDir, WindDir9am, WindDir3pm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>compas</td>\n",
       "      <td>compas-scores-two-years.csv</td>\n",
       "      <td>binary</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>two_year_recid</td>\n",
       "      <td>,</td>\n",
       "      <td>shared</td>\n",
       "      <td>[age, juv_fel_count, juv_misd_count, juv_other...</td>\n",
       "      <td>[sex, age_cat, race, c_charge_degree, c_charge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>car</td>\n",
       "      <td>car.data</td>\n",
       "      <td>regression</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>price</td>\n",
       "      <td>,</td>\n",
       "      <td>shared</td>\n",
       "      <td>[wheel-base, length, width, height, curb-weigh...</td>\n",
       "      <td>[symboling, make, fuel-type, aspiration, num-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>student</td>\n",
       "      <td>student-por.csv</td>\n",
       "      <td>regression</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>G3</td>\n",
       "      <td>;</td>\n",
       "      <td>shared</td>\n",
       "      <td>[age, Medu, Fedu, traveltime, studytime, failu...</td>\n",
       "      <td>[school, sex, address, famsize, Pstatus, Mjob,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bike</td>\n",
       "      <td>bike.csv</td>\n",
       "      <td>regression</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>cnt</td>\n",
       "      <td>,</td>\n",
       "      <td>shared</td>\n",
       "      <td>[mnth, hr, temp, atemp, hum, windspeed, weekday]</td>\n",
       "      <td>[season, yr, holiday, workingday, weathersit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>insurance</td>\n",
       "      <td>insurance.csv</td>\n",
       "      <td>regression</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>charges</td>\n",
       "      <td>,</td>\n",
       "      <td>shared</td>\n",
       "      <td>[age, bmi, children]</td>\n",
       "      <td>[sex, smoker, region]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>crab</td>\n",
       "      <td>CrabAgePrediction.csv</td>\n",
       "      <td>regression</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Age</td>\n",
       "      <td>,</td>\n",
       "      <td>shared</td>\n",
       "      <td>[Length, Diameter, Height, Weight, Shucked Wei...</td>\n",
       "      <td>[Sex]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>diamond</td>\n",
       "      <td>Diamonds Prices2022.csv</td>\n",
       "      <td>regression</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>price</td>\n",
       "      <td>,</td>\n",
       "      <td>shared</td>\n",
       "      <td>[carat, depth, table, x, y, z]</td>\n",
       "      <td>[cut, color, clarity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>productivity</td>\n",
       "      <td>garments_worker_productivity.csv</td>\n",
       "      <td>regression</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>actual_productivity</td>\n",
       "      <td>,</td>\n",
       "      <td>shared</td>\n",
       "      <td>[targeted_productivity, smv, wip, over_time, i...</td>\n",
       "      <td>[quarter, department, day, team]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>diabetes</td>\n",
       "      <td>diabetes.csv</td>\n",
       "      <td>regression</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Progression</td>\n",
       "      <td>,</td>\n",
       "      <td>shared</td>\n",
       "      <td>[Age, Bmi, Bp, Tc, Ldl, Hdl, Tch, Ltg, Glu]</td>\n",
       "      <td>[Sex]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name                             file_name   task_type  \\\n",
       "0         stroke    healthcare-dataset-stroke-data.csv      binary   \n",
       "1          churn  WA_Fn-UseC_-Telco-Customer-Churn.csv      binary   \n",
       "2           fico             fico_heloc_dataset_v1.csv      binary   \n",
       "3           bank                         bank-full.csv      binary   \n",
       "4          adult               adult_census_income.csv      binary   \n",
       "5        airline                     airline_train.csv      binary   \n",
       "6        college                      college_data.csv      binary   \n",
       "7        weather                        weatherAUS.csv      binary   \n",
       "8         compas           compas-scores-two-years.csv      binary   \n",
       "9            car                              car.data  regression   \n",
       "10       student                       student-por.csv  regression   \n",
       "11          bike                              bike.csv  regression   \n",
       "12     insurance                         insurance.csv  regression   \n",
       "13          crab                 CrabAgePrediction.csv  regression   \n",
       "14       diamond               Diamonds Prices2022.csv  regression   \n",
       "15  productivity      garments_worker_productivity.csv  regression   \n",
       "16      diabetes                          diabetes.csv  regression   \n",
       "\n",
       "    n_num_features  n_cat_features               target separator folder_name  \\\n",
       "0                3               7               stroke         ,      shared   \n",
       "1                3              16                Churn         ,      shared   \n",
       "2               21               2      RiskPerformance         ,      shared   \n",
       "3                6               9                    y         ;      shared   \n",
       "4                6               8               income         ,      shared   \n",
       "5               18               4         satisfaction         ,      shared   \n",
       "6                4               6   will_go_to_college         ,      shared   \n",
       "7               16               5         RainTomorrow         ,      shared   \n",
       "8                7               5       two_year_recid         ,      shared   \n",
       "9               13              11                price         ,      shared   \n",
       "10              13              17                   G3         ;      shared   \n",
       "11               7               5                  cnt         ,      shared   \n",
       "12               3               3              charges         ,      shared   \n",
       "13               7               1                  Age         ,      shared   \n",
       "14               6               3                price         ,      shared   \n",
       "15               9               4  actual_productivity         ,      shared   \n",
       "16               9               1          Progression         ,      shared   \n",
       "\n",
       "                                         num_features  \\\n",
       "0                       [age, avg_glucose_level, bmi]   \n",
       "1              [tenure, MonthlyCharges, TotalCharges]   \n",
       "2   [ExternalRiskEstimate, MSinceOldestTradeOpen, ...   \n",
       "3      [age, balance, day, campaign, pdays, previous]   \n",
       "4   [age, fnlwgt, education.num, capital.gain, cap...   \n",
       "5   [Age, Flight Distance, Inflight wifi service, ...   \n",
       "6   [parent_age, parent_salary, house_area, averag...   \n",
       "7   [MinTemp, MaxTemp, Rainfall, Evaporation, Suns...   \n",
       "8   [age, juv_fel_count, juv_misd_count, juv_other...   \n",
       "9   [wheel-base, length, width, height, curb-weigh...   \n",
       "10  [age, Medu, Fedu, traveltime, studytime, failu...   \n",
       "11   [mnth, hr, temp, atemp, hum, windspeed, weekday]   \n",
       "12                               [age, bmi, children]   \n",
       "13  [Length, Diameter, Height, Weight, Shucked Wei...   \n",
       "14                     [carat, depth, table, x, y, z]   \n",
       "15  [targeted_productivity, smv, wip, over_time, i...   \n",
       "16        [Age, Bmi, Bp, Tc, Ldl, Hdl, Tch, Ltg, Glu]   \n",
       "\n",
       "                                         cat_features  \n",
       "0   [gender, hypertension, heart_disease, ever_mar...  \n",
       "1   [gender, SeniorCitizen, Partner, Dependents, P...  \n",
       "2             [MaxDelq2PublicRecLast12M, MaxDelqEver]  \n",
       "3   [job, marital, education, default, housing, lo...  \n",
       "4   [workclass, education, marital.status, occupat...  \n",
       "5      [Gender, Customer Type, Type of Travel, Class]  \n",
       "6   [type_school, school_accreditation, gender, in...  \n",
       "7   [Location, WindGustDir, WindDir9am, WindDir3pm...  \n",
       "8   [sex, age_cat, race, c_charge_degree, c_charge...  \n",
       "9   [symboling, make, fuel-type, aspiration, num-o...  \n",
       "10  [school, sex, address, famsize, Pstatus, Mjob,...  \n",
       "11      [season, yr, holiday, workingday, weathersit]  \n",
       "12                              [sex, smoker, region]  \n",
       "13                                              [Sex]  \n",
       "14                              [cut, color, clarity]  \n",
       "15                   [quarter, department, day, team]  \n",
       "16                                              [Sex]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"/Users/maximilianveitl/workspace/master/datasets-main/datasets.yml\", \"r\") as file:\n",
    "    data = yaml.safe_load(file)\n",
    "\n",
    "dataset_rows = []\n",
    "for name, cfg in data[\"datasets\"].items():\n",
    "    num_features = cfg.get(\"numerical_features\", [])\n",
    "    cat_features = cfg.get(\"categorical_features\", [])\n",
    "\n",
    "    if len(cat_features) >= 1:\n",
    "        row = {\n",
    "            \"name\": name,\n",
    "            \"file_name\": cfg.get(\"file_name\"),\n",
    "            \"task_type\": cfg.get(\"task_type\"),\n",
    "            \"n_num_features\": len(num_features),\n",
    "            \"n_cat_features\": len(cat_features),\n",
    "            \"target\": cfg.get(\"target\"),\n",
    "            \"separator\": cfg.get(\"separator\", \",\"),\n",
    "            \"folder_name\": cfg.get(\"folder_name\", \"shared\"),\n",
    "            \"num_features\": num_features,\n",
    "            \"cat_features\": cat_features\n",
    "        }\n",
    "        dataset_rows.append(row)\n",
    "        \n",
    "df_overview = pd.DataFrame(dataset_rows)\n",
    "\n",
    "df_overview.head(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20e0578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: stroke, Shape: (5110, 12)\n",
      "Dataset: churn, Shape: (7043, 21)\n",
      "Dataset: fico, Shape: (10459, 24)\n",
      "Dataset: bank, Shape: (45211, 17)\n",
      "Dataset: adult, Shape: (32561, 15)\n",
      "Dataset: airline, Shape: (103904, 25)\n",
      "Dataset: college, Shape: (1000, 11)\n",
      "Dataset: weather, Shape: (145460, 23)\n",
      "Dataset: compas, Shape: (7214, 53)\n",
      "Dataset: car, Shape: (201, 26)\n",
      "Dataset: student, Shape: (649, 33)\n",
      "Dataset: bike, Shape: (17379, 17)\n",
      "Dataset: insurance, Shape: (1338, 7)\n",
      "Dataset: crab, Shape: (3893, 9)\n",
      "Dataset: diamond, Shape: (53943, 10)\n",
      "Dataset: productivity, Shape: (1197, 15)\n",
      "Dataset: diabetes, Shape: (442, 12)\n"
     ]
    }
   ],
   "source": [
    "# Shape of the datasets\n",
    "for idx, row in df_overview.iterrows():\n",
    "    file_path = f\"{data_path}/{row['name']}/{row['file_name']}\"\n",
    "    data = pd.read_csv(file_path, sep=row[\"separator\"])\n",
    "    print(f\"Dataset: {row['name']}, Shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c161ff7",
   "metadata": {},
   "source": [
    "# 4. Evaluation Regression Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dfbc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_regression = pd.DataFrame(columns=[\"model\", \"dataset\", \"mse\", \"r2\", \"train_duration\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a1e84a",
   "metadata": {},
   "source": [
    "## 4.1 Lasso and Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d62bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Lasso on student dataset...\n",
      "Training Ridge on student dataset...\n",
      "Training Lasso on bike dataset...\n",
      "Training Ridge on bike dataset...\n",
      "Training Lasso on insurance dataset...\n",
      "Training Ridge on insurance dataset...\n",
      "Training Lasso on crab dataset...\n",
      "Training Ridge on crab dataset...\n",
      "Training Lasso on diamond dataset...\n",
      "Training Ridge on diamond dataset...\n",
      "Training Lasso on productivity dataset...\n",
      "Training Ridge on productivity dataset...\n",
      "Training Lasso on diabetes dataset...\n",
      "Training Ridge on diabetes dataset...\n"
     ]
    }
   ],
   "source": [
    "# model Lasso and Ridge (one-hot encoding needed)\n",
    "datasets_filtered = df_overview[(df_overview[\"task_type\"] == \"regression\") & (df_overview[\"name\"] != \"car\")]\n",
    "\n",
    "models = {\n",
    "    \"Lasso\": Lasso(alpha=0.01),\n",
    "    \"Ridge\": Ridge(alpha=0.01)\n",
    "}\n",
    "\n",
    "for idx, row in datasets_filtered.iterrows():\n",
    "    # load data\n",
    "    file_name = row[\"file_name\"]\n",
    "    separator = row[\"separator\"]\n",
    "    folder_name = row[\"name\"]\n",
    "    target = row[\"target\"]\n",
    "    file_path = f\"{data_path}/{folder_name}/{file_name}\"\n",
    "\n",
    "    data = pd.read_csv(file_path, sep=separator)\n",
    "    \n",
    "    X = data.drop(columns=target)\n",
    "    y = data[target]\n",
    "\n",
    "    # Preprocessing\n",
    "    # 1. Train-Test-Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
    "\n",
    "    # 2. Split features into numerical and categorical\n",
    "    X_train_num = X_train[row[\"num_features\"]]\n",
    "    X_train_cat = X_train[row[\"cat_features\"]]\n",
    "    X_test_num = X_test[row[\"num_features\"]]\n",
    "    X_test_cat = X_test[row[\"cat_features\"]]\n",
    "\n",
    "    # 3. Replace empty strings and other chars in numerical features with np.nan\n",
    "    X_train_num = X_train_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "    X_test_num = X_test_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "\n",
    "    # 4. Correct data types\n",
    "    X_train_num = X_train_num.astype(float)\n",
    "    X_test_num = X_test_num.astype(float)\n",
    "    X_train_cat = X_train_cat.astype(str)\n",
    "    X_test_cat = X_test_cat.astype(str)\n",
    "\n",
    "    # 5. Impute missing values\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "    X_train_num = imputer_num.fit_transform(X_train_num)\n",
    "    X_train_cat = imputer_cat.fit_transform(X_train_cat)\n",
    "    X_test_num = imputer_num.transform(X_test_num)\n",
    "    X_test_cat = imputer_cat.transform(X_test_cat)\n",
    "\n",
    "    # 6. Scale numerical features and target and one-hot encode categorical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train_num)\n",
    "    X_test_num = scaler.transform(X_test_num)\n",
    "    y_train_mean = y_train.mean()\n",
    "    y_train_std = y_train.std()\n",
    "    y_train = (y_train - y_train_mean) / y_train_std\n",
    "    y_test = (y_test - y_train_mean) / y_train_std\n",
    "    X_train_cat = pd.DataFrame(X_train_cat, columns=row[\"cat_features\"])\n",
    "    X_test_cat = pd.DataFrame(X_test_cat, columns=row[\"cat_features\"])\n",
    "    X_train_cat = pd.get_dummies(X_train_cat, drop_first=True)\n",
    "    X_test_cat = pd.get_dummies(X_test_cat, drop_first=True)\n",
    "    X_test_cat = X_test_cat.reindex(columns=X_train_cat.columns, fill_value=0)\n",
    "\n",
    "    # 7. Concatenate numerical and categorical features\n",
    "    X_train_num = pd.DataFrame(X_train_num, columns=row[\"num_features\"])\n",
    "    X_test_num = pd.DataFrame(X_test_num, columns=row[\"num_features\"])\n",
    "\n",
    "    X_train = pd.concat([X_train_num, X_train_cat], axis=1)\n",
    "    X_test = pd.concat([X_test_num, X_test_cat], axis=1)\n",
    "\n",
    "    # 8. Train Models\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name} on {folder_name} dataset...\")\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_duration = time.time() - start_time\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # 9. Evaluate Models\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        results_regression.loc[len(results_regression)] = {\n",
    "            'model': name,\n",
    "            'dataset': folder_name,\n",
    "            'mse': mse,\n",
    "            'r2': r2,\n",
    "            'train_duration': train_duration}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d588a6bd",
   "metadata": {},
   "source": [
    "## 4.2 Decision Tree and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b6f3ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DecisionTree on student dataset...\n",
      "Training RandomForest on student dataset...\n",
      "Training DecisionTree on bike dataset...\n",
      "Training RandomForest on bike dataset...\n",
      "Training DecisionTree on insurance dataset...\n",
      "Training RandomForest on insurance dataset...\n",
      "Training DecisionTree on crab dataset...\n",
      "Training RandomForest on crab dataset...\n",
      "Training DecisionTree on diamond dataset...\n",
      "Training RandomForest on diamond dataset...\n",
      "Training DecisionTree on productivity dataset...\n",
      "Training RandomForest on productivity dataset...\n",
      "Training DecisionTree on diabetes dataset...\n",
      "Training RandomForest on diabetes dataset...\n"
     ]
    }
   ],
   "source": [
    "# DecisionTree, RandomForest (OrdinalEncoder needed)\n",
    "datasets_filtered = df_overview[(df_overview[\"task_type\"] == \"regression\") & (df_overview[\"name\"] != \"car\")]\n",
    "\n",
    "models = {\n",
    "    \"DecisionTree\": DecisionTreeRegressor(max_depth=5, random_state=seed),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, max_depth=10, random_state=seed),\n",
    "}\n",
    "\n",
    "for idx, row in datasets_filtered.iterrows():\n",
    "    # load data\n",
    "    file_name = row[\"file_name\"]\n",
    "    separator = row[\"separator\"]\n",
    "    folder_name = row[\"name\"]\n",
    "    target = row[\"target\"]\n",
    "    file_path = f\"{data_path}/{folder_name}/{file_name}\"\n",
    "\n",
    "    data = pd.read_csv(file_path, sep=separator)\n",
    "    \n",
    "    X = data.drop(columns=target)\n",
    "    y = data[target]\n",
    "\n",
    "    # Preprocessing\n",
    "    # 1. Train-Test-Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
    "\n",
    "    # 2. Split features into numerical and categorical\n",
    "    X_train_num = X_train[row[\"num_features\"]]\n",
    "    X_train_cat = X_train[row[\"cat_features\"]]\n",
    "    X_test_num = X_test[row[\"num_features\"]]\n",
    "    X_test_cat = X_test[row[\"cat_features\"]]\n",
    "\n",
    "    # 3. Replace empty strings and other chars in numerical features with np.nan\n",
    "    X_train_num = X_train_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "    X_test_num = X_test_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "\n",
    "    # 4. Correct data types\n",
    "    X_train_num = X_train_num.astype(float)\n",
    "    X_test_num = X_test_num.astype(float)\n",
    "    X_train_cat = X_train_cat.astype(str)\n",
    "    X_test_cat = X_test_cat.astype(str)\n",
    "\n",
    "    # 5. Impute missing values\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "    X_train_num = imputer_num.fit_transform(X_train_num)\n",
    "    X_train_cat = imputer_cat.fit_transform(X_train_cat)\n",
    "    X_test_num = imputer_num.transform(X_test_num)\n",
    "    X_test_cat = imputer_cat.transform(X_test_cat)\n",
    "\n",
    "    # 6. Scale numerical features and target and ordinal encoding categorical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train_num)\n",
    "    X_test_num = scaler.transform(X_test_num)\n",
    "    y_train_mean = y_train.mean()\n",
    "    y_train_std = y_train.std()\n",
    "    y_train = (y_train - y_train_mean) / y_train_std\n",
    "    y_test = (y_test - y_train_mean) / y_train_std\n",
    "    X_train_cat = pd.DataFrame(X_train_cat, columns=row[\"cat_features\"])\n",
    "    X_test_cat = pd.DataFrame(X_test_cat, columns=row[\"cat_features\"])\n",
    "    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    X_train_cat = encoder.fit_transform(X_train_cat)\n",
    "    X_test_cat = encoder.transform(X_test_cat)\n",
    "\n",
    "    # 7. Concatenate numerical and categorical features\n",
    "    X_train_num = pd.DataFrame(X_train_num, columns=row[\"num_features\"])\n",
    "    X_test_num = pd.DataFrame(X_test_num, columns=row[\"num_features\"])\n",
    "    X_train_cat = pd.DataFrame(X_train_cat, columns=row[\"cat_features\"])\n",
    "    X_test_cat = pd.DataFrame(X_test_cat, columns=row[\"cat_features\"])\n",
    "\n",
    "    X_train = pd.concat([X_train_num, X_train_cat], axis=1)\n",
    "    X_test = pd.concat([X_test_num, X_test_cat], axis=1)\n",
    "        \n",
    "    # 8. Train Models\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name} on {folder_name} dataset...\")\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_duration = time.time() - start_time\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # 9. Evaluate Models\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        results_regression.loc[len(results_regression)] = {\n",
    "            'model': name,\n",
    "            'dataset': folder_name,\n",
    "            'mse': mse,\n",
    "            'r2': r2,\n",
    "            'train_duration': train_duration}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456ee62",
   "metadata": {},
   "source": [
    "## 4.3 EBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d64109d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EBM on student dataset...\n",
      "Training EBM on bike dataset...\n",
      "Training EBM on insurance dataset...\n",
      "Training EBM on crab dataset...\n",
      "Training EBM on diamond dataset...\n",
      "Training EBM on productivity dataset...\n",
      "Training EBM on diabetes dataset...\n"
     ]
    }
   ],
   "source": [
    "# EBM\n",
    "\n",
    "datasets_filtered = df_overview[(df_overview[\"task_type\"] == \"regression\") & (df_overview[\"name\"] != \"car\")]\n",
    "\n",
    "for idx, row in datasets_filtered.iterrows():\n",
    "    # load data\n",
    "    file_name = row[\"file_name\"]\n",
    "    separator = row[\"separator\"]\n",
    "    folder_name = row[\"name\"]\n",
    "    target = row[\"target\"]\n",
    "    file_path = f\"{data_path}/{folder_name}/{file_name}\"\n",
    "\n",
    "    data = pd.read_csv(file_path, sep=separator)\n",
    "    \n",
    "    X = data.drop(columns=target)\n",
    "    y = data[target]\n",
    "\n",
    "    # Preprocessing\n",
    "    # 1. Train-Test-Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
    "\n",
    "    # 2. Split features into numerical and categorical\n",
    "    X_train_num = X_train[row[\"num_features\"]]\n",
    "    X_train_cat = X_train[row[\"cat_features\"]]\n",
    "    X_test_num = X_test[row[\"num_features\"]]\n",
    "    X_test_cat = X_test[row[\"cat_features\"]]\n",
    "\n",
    "    # 3. Replace empty strings and other chars in numerical features with np.nan\n",
    "    X_train_num = X_train_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "    X_test_num = X_test_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "\n",
    "    # 4. Correct data types\n",
    "    X_train_num = X_train_num.astype(float)\n",
    "    X_test_num = X_test_num.astype(float)\n",
    "    X_train_cat = X_train_cat.astype(str)\n",
    "    X_test_cat = X_test_cat.astype(str)\n",
    "\n",
    "    # 5. Impute missing values\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "    X_train_num = imputer_num.fit_transform(X_train_num)\n",
    "    X_train_cat = imputer_cat.fit_transform(X_train_cat)\n",
    "    X_test_num = imputer_num.transform(X_test_num)\n",
    "    X_test_cat = imputer_cat.transform(X_test_cat)\n",
    "\n",
    "    # 6. Scale numerical features and target and one-hot encode categorical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train_num)\n",
    "    X_test_num = scaler.transform(X_test_num)\n",
    "    y_train_mean = y_train.mean()\n",
    "    y_train_std = y_train.std()\n",
    "    y_train = (y_train - y_train_mean) / y_train_std\n",
    "    y_test = (y_test - y_train_mean) / y_train_std\n",
    "\n",
    "    # 7. Concatenate numerical and categorical features\n",
    "    X_train_num = pd.DataFrame(X_train_num, columns=row[\"num_features\"])\n",
    "    X_test_num = pd.DataFrame(X_test_num, columns=row[\"num_features\"])\n",
    "    X_train_cat = pd.DataFrame(X_train_cat, columns=row[\"cat_features\"])\n",
    "    X_test_cat = pd.DataFrame(X_test_cat, columns=row[\"cat_features\"])\n",
    "\n",
    "    X_train = pd.concat([X_train_num, X_train_cat], axis=1)\n",
    "    X_test = pd.concat([X_test_num, X_test_cat], axis=1)\n",
    "\n",
    "    # 8. Train Model\n",
    "    model = ExplainableBoostingRegressor(interactions=1, max_bins=64, learning_rate=0.1, max_leaves=3, min_samples_leaf=2, random_state=seed)\n",
    "    print(f\"Training EBM on {folder_name} dataset...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_duration = time.time() - start_time\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 9. Evaluate Model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results_regression.loc[len(results_regression)] = {\n",
    "        'model': 'EBM',\n",
    "        'dataset': folder_name,\n",
    "        'mse': mse,\n",
    "        'r2': r2,\n",
    "        'train_duration': train_duration}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dabcde",
   "metadata": {},
   "source": [
    "## 4.4 IGANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8059f5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IGANN on student dataset...\n",
      "Training IGANN on bike dataset...\n",
      "Training IGANN on insurance dataset...\n",
      "Training IGANN on crab dataset...\n",
      "Training IGANN on diamond dataset...\n",
      "Training IGANN on productivity dataset...\n",
      "Training IGANN on diabetes dataset...\n"
     ]
    }
   ],
   "source": [
    "# IGANN\n",
    "\n",
    "datasets_filtered = df_overview[(df_overview[\"task_type\"] == \"regression\") & (df_overview[\"name\"] != \"car\")]\n",
    "\n",
    "for idx, row in datasets_filtered.iterrows():\n",
    "    # load data\n",
    "    file_name = row[\"file_name\"]\n",
    "    separator = row[\"separator\"]\n",
    "    folder_name = row[\"name\"]\n",
    "    target = row[\"target\"]\n",
    "    file_path = f\"{data_path}/{folder_name}/{file_name}\"\n",
    "\n",
    "    data = pd.read_csv(file_path, sep=separator)\n",
    "    \n",
    "    X = data.drop(columns=target)\n",
    "    y = data[target]\n",
    "\n",
    "    # Preprocessing\n",
    "    # 1. Train-Test-Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
    "\n",
    "    # 2. Split features into numerical and categorical\n",
    "    X_train_num = X_train[row[\"num_features\"]]\n",
    "    X_train_cat = X_train[row[\"cat_features\"]]\n",
    "    X_test_num = X_test[row[\"num_features\"]]\n",
    "    X_test_cat = X_test[row[\"cat_features\"]]\n",
    "\n",
    "    # 3. Replace empty strings and other chars in numerical features with np.nan\n",
    "    X_train_num = X_train_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "    X_test_num = X_test_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "\n",
    "    # 4. Correct data types\n",
    "    X_train_num = X_train_num.astype(float)\n",
    "    X_test_num = X_test_num.astype(float)\n",
    "    X_train_cat = X_train_cat.astype(str)\n",
    "    X_test_cat = X_test_cat.astype(str)\n",
    "\n",
    "    # 5. Impute missing values\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "    X_train_num = imputer_num.fit_transform(X_train_num)\n",
    "    X_train_cat = imputer_cat.fit_transform(X_train_cat)\n",
    "    X_test_num = imputer_num.transform(X_test_num)\n",
    "    X_test_cat = imputer_cat.transform(X_test_cat)\n",
    "\n",
    "    # 6. Scale numerical features and target and one-hot encode categorical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train_num)\n",
    "    X_test_num = scaler.transform(X_test_num)\n",
    "    y_train_mean = y_train.mean()\n",
    "    y_train_std = y_train.std()\n",
    "    y_train = (y_train - y_train_mean) / y_train_std\n",
    "    y_test = (y_test - y_train_mean) / y_train_std\n",
    "\n",
    "    # 7. Concatenate numerical and categorical features\n",
    "    X_train_num = pd.DataFrame(X_train_num, columns=row[\"num_features\"])\n",
    "    X_test_num = pd.DataFrame(X_test_num, columns=row[\"num_features\"])\n",
    "    X_train_cat = pd.DataFrame(X_train_cat, columns=row[\"cat_features\"])\n",
    "    X_test_cat = pd.DataFrame(X_test_cat, columns=row[\"cat_features\"])\n",
    "\n",
    "    X_train = pd.concat([X_train_num, X_train_cat], axis=1)\n",
    "    X_test = pd.concat([X_test_num, X_test_cat], axis=1)\n",
    "\n",
    "    # 8. Train Model\n",
    "    print(f\"Training IGANN on {folder_name} dataset...\")\n",
    "    model = IGANN(task=\"regression\", n_hid=10, igann_it=False)\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_duration = time.time() - start_time\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 9. Evaluate Model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results_regression.loc[len(results_regression)] = {\n",
    "        'model': 'IGANN',\n",
    "        'dataset': folder_name,\n",
    "        'mse': mse,\n",
    "        'r2': r2,\n",
    "        'train_duration': train_duration}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8faadd8",
   "metadata": {},
   "source": [
    "## 4.5 IGANN-IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b1a52d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IGANN on student dataset...\n",
      "No feature combination found. Model does not capture interactions. Try different feature interaction detection method.\n",
      "Training IGANN on bike dataset...\n",
      "Training IGANN on insurance dataset...\n",
      "Training IGANN on crab dataset...\n",
      "Training IGANN on diamond dataset...\n",
      "Training IGANN on productivity dataset...\n",
      "Training IGANN on diabetes dataset...\n"
     ]
    }
   ],
   "source": [
    "# IGANN-IT\n",
    "\n",
    "datasets_filtered = df_overview[(df_overview[\"task_type\"] == \"regression\") & (df_overview[\"name\"] != \"car\")]\n",
    "\n",
    "for idx, row in datasets_filtered.iterrows():\n",
    "    # load data\n",
    "    file_name = row[\"file_name\"]\n",
    "    separator = row[\"separator\"]\n",
    "    folder_name = row[\"name\"]\n",
    "    target = row[\"target\"]\n",
    "    file_path = f\"{data_path}/{folder_name}/{file_name}\"\n",
    "\n",
    "    data = pd.read_csv(file_path, sep=separator)\n",
    "    \n",
    "    X = data.drop(columns=target)\n",
    "    y = data[target]\n",
    "\n",
    "    # Preprocessing\n",
    "    # 1. Train-Test-Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
    "\n",
    "    # 2. Split features into numerical and categorical\n",
    "    X_train_num = X_train[row[\"num_features\"]]\n",
    "    X_train_cat = X_train[row[\"cat_features\"]]\n",
    "    X_test_num = X_test[row[\"num_features\"]]\n",
    "    X_test_cat = X_test[row[\"cat_features\"]]\n",
    "\n",
    "    # 3. Replace empty strings and other chars in numerical features with np.nan\n",
    "    X_train_num = X_train_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "    X_test_num = X_test_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "\n",
    "    # 4. Correct data types\n",
    "    X_train_num = X_train_num.astype(float)\n",
    "    X_test_num = X_test_num.astype(float)\n",
    "    X_train_cat = X_train_cat.astype(str)\n",
    "    X_test_cat = X_test_cat.astype(str)\n",
    "\n",
    "    # 5. Impute missing values\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "    X_train_num = imputer_num.fit_transform(X_train_num)\n",
    "    X_train_cat = imputer_cat.fit_transform(X_train_cat)\n",
    "    X_test_num = imputer_num.transform(X_test_num)\n",
    "    X_test_cat = imputer_cat.transform(X_test_cat)\n",
    "\n",
    "    # 6. Scale numerical features and target and one-hot encode categorical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train_num)\n",
    "    X_test_num = scaler.transform(X_test_num)\n",
    "    y_train_mean = y_train.mean()\n",
    "    y_train_std = y_train.std()\n",
    "    y_train = (y_train - y_train_mean) / y_train_std\n",
    "    y_test = (y_test - y_train_mean) / y_train_std\n",
    "\n",
    "    # 7. Concatenate numerical and categorical features\n",
    "    X_train_num = pd.DataFrame(X_train_num, columns=row[\"num_features\"])\n",
    "    X_test_num = pd.DataFrame(X_test_num, columns=row[\"num_features\"])\n",
    "    X_train_cat = pd.DataFrame(X_train_cat, columns=row[\"cat_features\"])\n",
    "    X_test_cat = pd.DataFrame(X_test_cat, columns=row[\"cat_features\"])\n",
    "\n",
    "    X_train = pd.concat([X_train_num, X_train_cat], axis=1)\n",
    "    X_test = pd.concat([X_test_num, X_test_cat], axis=1)\n",
    "\n",
    "    # 8. Train Model\n",
    "    print(f\"Training IGANN on {folder_name} dataset...\")\n",
    "    model = IGANN(task=\"regression\", n_hid=10, interaction_detection_method=\"rulefit\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_duration = time.time() - start_time\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 9. Evaluate Model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results_regression.loc[len(results_regression)] = {\n",
    "        'model': 'IGANN-IT',\n",
    "        'dataset': folder_name,\n",
    "        'mse': mse,\n",
    "        'r2': r2,\n",
    "        'train_duration': train_duration}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f65d0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>mse</th>\n",
       "      <th>r2</th>\n",
       "      <th>train_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>student</td>\n",
       "      <td>0.767566</td>\n",
       "      <td>0.199821</td>\n",
       "      <td>0.003276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>student</td>\n",
       "      <td>0.784711</td>\n",
       "      <td>0.181947</td>\n",
       "      <td>0.002550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>bike</td>\n",
       "      <td>0.591985</td>\n",
       "      <td>0.389836</td>\n",
       "      <td>0.029584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>bike</td>\n",
       "      <td>0.586052</td>\n",
       "      <td>0.395951</td>\n",
       "      <td>0.004884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>insurance</td>\n",
       "      <td>0.245697</td>\n",
       "      <td>0.763513</td>\n",
       "      <td>0.003799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>insurance</td>\n",
       "      <td>0.241802</td>\n",
       "      <td>0.767262</td>\n",
       "      <td>0.002962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>crab</td>\n",
       "      <td>0.485787</td>\n",
       "      <td>0.484257</td>\n",
       "      <td>0.007466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>crab</td>\n",
       "      <td>0.452826</td>\n",
       "      <td>0.519251</td>\n",
       "      <td>0.003008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>diamond</td>\n",
       "      <td>0.108228</td>\n",
       "      <td>0.887382</td>\n",
       "      <td>0.039186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>diamond</td>\n",
       "      <td>0.074707</td>\n",
       "      <td>0.922263</td>\n",
       "      <td>0.013417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>productivity</td>\n",
       "      <td>0.738891</td>\n",
       "      <td>0.181135</td>\n",
       "      <td>0.005316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>productivity</td>\n",
       "      <td>0.778256</td>\n",
       "      <td>0.137510</td>\n",
       "      <td>0.003322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>diabetes</td>\n",
       "      <td>0.461463</td>\n",
       "      <td>0.494037</td>\n",
       "      <td>0.003180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>diabetes</td>\n",
       "      <td>0.469776</td>\n",
       "      <td>0.484921</td>\n",
       "      <td>0.002117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>student</td>\n",
       "      <td>0.871822</td>\n",
       "      <td>0.091135</td>\n",
       "      <td>0.003051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>student</td>\n",
       "      <td>0.814498</td>\n",
       "      <td>0.150895</td>\n",
       "      <td>0.290999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>bike</td>\n",
       "      <td>0.346287</td>\n",
       "      <td>0.643079</td>\n",
       "      <td>0.017874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>bike</td>\n",
       "      <td>0.076867</td>\n",
       "      <td>0.920773</td>\n",
       "      <td>2.475679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>insurance</td>\n",
       "      <td>0.160075</td>\n",
       "      <td>0.845925</td>\n",
       "      <td>0.002749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>insurance</td>\n",
       "      <td>0.156800</td>\n",
       "      <td>0.849078</td>\n",
       "      <td>0.285077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>crab</td>\n",
       "      <td>0.506275</td>\n",
       "      <td>0.462506</td>\n",
       "      <td>0.008155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>crab</td>\n",
       "      <td>0.440953</td>\n",
       "      <td>0.531856</td>\n",
       "      <td>1.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>diamond</td>\n",
       "      <td>0.067993</td>\n",
       "      <td>0.929249</td>\n",
       "      <td>0.058551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>diamond</td>\n",
       "      <td>0.019196</td>\n",
       "      <td>0.980025</td>\n",
       "      <td>7.887241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>productivity</td>\n",
       "      <td>0.564664</td>\n",
       "      <td>0.374220</td>\n",
       "      <td>0.003563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>productivity</td>\n",
       "      <td>0.461513</td>\n",
       "      <td>0.488536</td>\n",
       "      <td>0.329367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>diabetes</td>\n",
       "      <td>0.706907</td>\n",
       "      <td>0.224923</td>\n",
       "      <td>0.002927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>diabetes</td>\n",
       "      <td>0.495727</td>\n",
       "      <td>0.456468</td>\n",
       "      <td>0.257446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>EBM</td>\n",
       "      <td>student</td>\n",
       "      <td>0.749625</td>\n",
       "      <td>0.218524</td>\n",
       "      <td>10.150442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>EBM</td>\n",
       "      <td>bike</td>\n",
       "      <td>0.162454</td>\n",
       "      <td>0.832557</td>\n",
       "      <td>4.802202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>EBM</td>\n",
       "      <td>insurance</td>\n",
       "      <td>0.131755</td>\n",
       "      <td>0.873184</td>\n",
       "      <td>1.168456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>EBM</td>\n",
       "      <td>crab</td>\n",
       "      <td>0.447842</td>\n",
       "      <td>0.524542</td>\n",
       "      <td>1.880723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>EBM</td>\n",
       "      <td>diamond</td>\n",
       "      <td>0.040074</td>\n",
       "      <td>0.958301</td>\n",
       "      <td>21.923022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>EBM</td>\n",
       "      <td>productivity</td>\n",
       "      <td>0.542617</td>\n",
       "      <td>0.398653</td>\n",
       "      <td>1.981410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>EBM</td>\n",
       "      <td>diabetes</td>\n",
       "      <td>0.463721</td>\n",
       "      <td>0.491560</td>\n",
       "      <td>1.693768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>IGANN</td>\n",
       "      <td>student</td>\n",
       "      <td>0.739720</td>\n",
       "      <td>0.228850</td>\n",
       "      <td>0.180249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>IGANN</td>\n",
       "      <td>bike</td>\n",
       "      <td>0.393025</td>\n",
       "      <td>0.594905</td>\n",
       "      <td>10.725619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>IGANN</td>\n",
       "      <td>insurance</td>\n",
       "      <td>0.250985</td>\n",
       "      <td>0.758423</td>\n",
       "      <td>0.433337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>IGANN</td>\n",
       "      <td>crab</td>\n",
       "      <td>0.432301</td>\n",
       "      <td>0.541041</td>\n",
       "      <td>0.986557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>IGANN</td>\n",
       "      <td>diamond</td>\n",
       "      <td>0.073215</td>\n",
       "      <td>0.923816</td>\n",
       "      <td>1.887702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>IGANN</td>\n",
       "      <td>productivity</td>\n",
       "      <td>0.653038</td>\n",
       "      <td>0.276280</td>\n",
       "      <td>0.908650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>IGANN</td>\n",
       "      <td>diabetes</td>\n",
       "      <td>0.466449</td>\n",
       "      <td>0.488570</td>\n",
       "      <td>0.765108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>IGANN-IT</td>\n",
       "      <td>student</td>\n",
       "      <td>0.739720</td>\n",
       "      <td>0.228850</td>\n",
       "      <td>0.681804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>IGANN-IT</td>\n",
       "      <td>bike</td>\n",
       "      <td>0.370538</td>\n",
       "      <td>0.618083</td>\n",
       "      <td>17.286643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>IGANN-IT</td>\n",
       "      <td>insurance</td>\n",
       "      <td>0.143847</td>\n",
       "      <td>0.861545</td>\n",
       "      <td>2.845880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>IGANN-IT</td>\n",
       "      <td>crab</td>\n",
       "      <td>0.429943</td>\n",
       "      <td>0.543545</td>\n",
       "      <td>2.491910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>IGANN-IT</td>\n",
       "      <td>diamond</td>\n",
       "      <td>0.083083</td>\n",
       "      <td>0.913547</td>\n",
       "      <td>8.639091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>IGANN-IT</td>\n",
       "      <td>productivity</td>\n",
       "      <td>0.653038</td>\n",
       "      <td>0.276280</td>\n",
       "      <td>1.383201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>IGANN-IT</td>\n",
       "      <td>diabetes</td>\n",
       "      <td>0.466449</td>\n",
       "      <td>0.488570</td>\n",
       "      <td>1.053885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model       dataset       mse        r2  train_duration\n",
       "0          Lasso       student  0.767566  0.199821        0.003276\n",
       "1          Ridge       student  0.784711  0.181947        0.002550\n",
       "2          Lasso          bike  0.591985  0.389836        0.029584\n",
       "3          Ridge          bike  0.586052  0.395951        0.004884\n",
       "4          Lasso     insurance  0.245697  0.763513        0.003799\n",
       "5          Ridge     insurance  0.241802  0.767262        0.002962\n",
       "6          Lasso          crab  0.485787  0.484257        0.007466\n",
       "7          Ridge          crab  0.452826  0.519251        0.003008\n",
       "8          Lasso       diamond  0.108228  0.887382        0.039186\n",
       "9          Ridge       diamond  0.074707  0.922263        0.013417\n",
       "10         Lasso  productivity  0.738891  0.181135        0.005316\n",
       "11         Ridge  productivity  0.778256  0.137510        0.003322\n",
       "12         Lasso      diabetes  0.461463  0.494037        0.003180\n",
       "13         Ridge      diabetes  0.469776  0.484921        0.002117\n",
       "14  DecisionTree       student  0.871822  0.091135        0.003051\n",
       "15  RandomForest       student  0.814498  0.150895        0.290999\n",
       "16  DecisionTree          bike  0.346287  0.643079        0.017874\n",
       "17  RandomForest          bike  0.076867  0.920773        2.475679\n",
       "18  DecisionTree     insurance  0.160075  0.845925        0.002749\n",
       "19  RandomForest     insurance  0.156800  0.849078        0.285077\n",
       "20  DecisionTree          crab  0.506275  0.462506        0.008155\n",
       "21  RandomForest          crab  0.440953  0.531856        1.110600\n",
       "22  DecisionTree       diamond  0.067993  0.929249        0.058551\n",
       "23  RandomForest       diamond  0.019196  0.980025        7.887241\n",
       "24  DecisionTree  productivity  0.564664  0.374220        0.003563\n",
       "25  RandomForest  productivity  0.461513  0.488536        0.329367\n",
       "26  DecisionTree      diabetes  0.706907  0.224923        0.002927\n",
       "27  RandomForest      diabetes  0.495727  0.456468        0.257446\n",
       "28           EBM       student  0.749625  0.218524       10.150442\n",
       "29           EBM          bike  0.162454  0.832557        4.802202\n",
       "30           EBM     insurance  0.131755  0.873184        1.168456\n",
       "31           EBM          crab  0.447842  0.524542        1.880723\n",
       "32           EBM       diamond  0.040074  0.958301       21.923022\n",
       "33           EBM  productivity  0.542617  0.398653        1.981410\n",
       "34           EBM      diabetes  0.463721  0.491560        1.693768\n",
       "35         IGANN       student  0.739720  0.228850        0.180249\n",
       "36         IGANN          bike  0.393025  0.594905       10.725619\n",
       "37         IGANN     insurance  0.250985  0.758423        0.433337\n",
       "38         IGANN          crab  0.432301  0.541041        0.986557\n",
       "39         IGANN       diamond  0.073215  0.923816        1.887702\n",
       "40         IGANN  productivity  0.653038  0.276280        0.908650\n",
       "41         IGANN      diabetes  0.466449  0.488570        0.765108\n",
       "42      IGANN-IT       student  0.739720  0.228850        0.681804\n",
       "43      IGANN-IT          bike  0.370538  0.618083       17.286643\n",
       "44      IGANN-IT     insurance  0.143847  0.861545        2.845880\n",
       "45      IGANN-IT          crab  0.429943  0.543545        2.491910\n",
       "46      IGANN-IT       diamond  0.083083  0.913547        8.639091\n",
       "47      IGANN-IT  productivity  0.653038  0.276280        1.383201\n",
       "48      IGANN-IT      diabetes  0.466449  0.488570        1.053885"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7462131",
   "metadata": {},
   "source": [
    "## 4.5 Results Regression Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911462cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metric_df(metric):\n",
    "    pivot_df = results_regression.pivot_table(index='model', columns='dataset', values=metric)\n",
    "    pivot_df['mean'] = pivot_df.mean(axis=1)\n",
    "    pivot_df = pivot_df[['mean'] + [col for col in pivot_df.columns if col != 'mean']]\n",
    "    return pivot_df\n",
    "\n",
    "mse_df = create_metric_df('mse')\n",
    "r2_df = create_metric_df('r2')\n",
    "train_time_df = create_metric_df('train_duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ab33192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset</th>\n",
       "      <th>mean</th>\n",
       "      <th>bike</th>\n",
       "      <th>crab</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>diamond</th>\n",
       "      <th>insurance</th>\n",
       "      <th>productivity</th>\n",
       "      <th>student</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>0.460575</td>\n",
       "      <td>0.346287</td>\n",
       "      <td>0.506275</td>\n",
       "      <td>0.706907</td>\n",
       "      <td>0.067993</td>\n",
       "      <td>0.160075</td>\n",
       "      <td>0.564664</td>\n",
       "      <td>0.871822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EBM</th>\n",
       "      <td>0.362584</td>\n",
       "      <td>0.162454</td>\n",
       "      <td>0.447842</td>\n",
       "      <td>0.463721</td>\n",
       "      <td>0.040074</td>\n",
       "      <td>0.131755</td>\n",
       "      <td>0.542617</td>\n",
       "      <td>0.749625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IGANN</th>\n",
       "      <td>0.429819</td>\n",
       "      <td>0.393025</td>\n",
       "      <td>0.432301</td>\n",
       "      <td>0.466449</td>\n",
       "      <td>0.073215</td>\n",
       "      <td>0.250985</td>\n",
       "      <td>0.653038</td>\n",
       "      <td>0.739720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IGANN-IT</th>\n",
       "      <td>0.412374</td>\n",
       "      <td>0.370538</td>\n",
       "      <td>0.429943</td>\n",
       "      <td>0.466449</td>\n",
       "      <td>0.083083</td>\n",
       "      <td>0.143847</td>\n",
       "      <td>0.653038</td>\n",
       "      <td>0.739720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.485660</td>\n",
       "      <td>0.591985</td>\n",
       "      <td>0.485787</td>\n",
       "      <td>0.461463</td>\n",
       "      <td>0.108228</td>\n",
       "      <td>0.245697</td>\n",
       "      <td>0.738891</td>\n",
       "      <td>0.767566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.352222</td>\n",
       "      <td>0.076867</td>\n",
       "      <td>0.440953</td>\n",
       "      <td>0.495727</td>\n",
       "      <td>0.019196</td>\n",
       "      <td>0.156800</td>\n",
       "      <td>0.461513</td>\n",
       "      <td>0.814498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.484018</td>\n",
       "      <td>0.586052</td>\n",
       "      <td>0.452826</td>\n",
       "      <td>0.469776</td>\n",
       "      <td>0.074707</td>\n",
       "      <td>0.241802</td>\n",
       "      <td>0.778256</td>\n",
       "      <td>0.784711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset           mean      bike      crab  diabetes   diamond  insurance  \\\n",
       "model                                                                       \n",
       "DecisionTree  0.460575  0.346287  0.506275  0.706907  0.067993   0.160075   \n",
       "EBM           0.362584  0.162454  0.447842  0.463721  0.040074   0.131755   \n",
       "IGANN         0.429819  0.393025  0.432301  0.466449  0.073215   0.250985   \n",
       "IGANN-IT      0.412374  0.370538  0.429943  0.466449  0.083083   0.143847   \n",
       "Lasso         0.485660  0.591985  0.485787  0.461463  0.108228   0.245697   \n",
       "RandomForest  0.352222  0.076867  0.440953  0.495727  0.019196   0.156800   \n",
       "Ridge         0.484018  0.586052  0.452826  0.469776  0.074707   0.241802   \n",
       "\n",
       "dataset       productivity   student  \n",
       "model                                 \n",
       "DecisionTree      0.564664  0.871822  \n",
       "EBM               0.542617  0.749625  \n",
       "IGANN             0.653038  0.739720  \n",
       "IGANN-IT          0.653038  0.739720  \n",
       "Lasso             0.738891  0.767566  \n",
       "RandomForest      0.461513  0.814498  \n",
       "Ridge             0.778256  0.784711  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2af7ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset</th>\n",
       "      <th>mean</th>\n",
       "      <th>bike</th>\n",
       "      <th>crab</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>diamond</th>\n",
       "      <th>insurance</th>\n",
       "      <th>productivity</th>\n",
       "      <th>student</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>0.510148</td>\n",
       "      <td>0.643079</td>\n",
       "      <td>0.462506</td>\n",
       "      <td>0.224923</td>\n",
       "      <td>0.929249</td>\n",
       "      <td>0.845925</td>\n",
       "      <td>0.374220</td>\n",
       "      <td>0.091135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EBM</th>\n",
       "      <td>0.613903</td>\n",
       "      <td>0.832557</td>\n",
       "      <td>0.524542</td>\n",
       "      <td>0.491560</td>\n",
       "      <td>0.958301</td>\n",
       "      <td>0.873184</td>\n",
       "      <td>0.398653</td>\n",
       "      <td>0.218524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IGANN</th>\n",
       "      <td>0.544555</td>\n",
       "      <td>0.594905</td>\n",
       "      <td>0.541041</td>\n",
       "      <td>0.488570</td>\n",
       "      <td>0.923816</td>\n",
       "      <td>0.758423</td>\n",
       "      <td>0.276280</td>\n",
       "      <td>0.228850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IGANN-IT</th>\n",
       "      <td>0.561489</td>\n",
       "      <td>0.618083</td>\n",
       "      <td>0.543545</td>\n",
       "      <td>0.488570</td>\n",
       "      <td>0.913547</td>\n",
       "      <td>0.861545</td>\n",
       "      <td>0.276280</td>\n",
       "      <td>0.228850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.485711</td>\n",
       "      <td>0.389836</td>\n",
       "      <td>0.484257</td>\n",
       "      <td>0.494037</td>\n",
       "      <td>0.887382</td>\n",
       "      <td>0.763513</td>\n",
       "      <td>0.181135</td>\n",
       "      <td>0.199821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.625376</td>\n",
       "      <td>0.920773</td>\n",
       "      <td>0.531856</td>\n",
       "      <td>0.456468</td>\n",
       "      <td>0.980025</td>\n",
       "      <td>0.849078</td>\n",
       "      <td>0.488536</td>\n",
       "      <td>0.150895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.487015</td>\n",
       "      <td>0.395951</td>\n",
       "      <td>0.519251</td>\n",
       "      <td>0.484921</td>\n",
       "      <td>0.922263</td>\n",
       "      <td>0.767262</td>\n",
       "      <td>0.137510</td>\n",
       "      <td>0.181947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset           mean      bike      crab  diabetes   diamond  insurance  \\\n",
       "model                                                                       \n",
       "DecisionTree  0.510148  0.643079  0.462506  0.224923  0.929249   0.845925   \n",
       "EBM           0.613903  0.832557  0.524542  0.491560  0.958301   0.873184   \n",
       "IGANN         0.544555  0.594905  0.541041  0.488570  0.923816   0.758423   \n",
       "IGANN-IT      0.561489  0.618083  0.543545  0.488570  0.913547   0.861545   \n",
       "Lasso         0.485711  0.389836  0.484257  0.494037  0.887382   0.763513   \n",
       "RandomForest  0.625376  0.920773  0.531856  0.456468  0.980025   0.849078   \n",
       "Ridge         0.487015  0.395951  0.519251  0.484921  0.922263   0.767262   \n",
       "\n",
       "dataset       productivity   student  \n",
       "model                                 \n",
       "DecisionTree      0.374220  0.091135  \n",
       "EBM               0.398653  0.218524  \n",
       "IGANN             0.276280  0.228850  \n",
       "IGANN-IT          0.276280  0.228850  \n",
       "Lasso             0.181135  0.199821  \n",
       "RandomForest      0.488536  0.150895  \n",
       "Ridge             0.137510  0.181947  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07feacc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset</th>\n",
       "      <th>mean</th>\n",
       "      <th>bike</th>\n",
       "      <th>crab</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>diamond</th>\n",
       "      <th>insurance</th>\n",
       "      <th>productivity</th>\n",
       "      <th>student</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>0.013839</td>\n",
       "      <td>0.017874</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.058551</td>\n",
       "      <td>0.002749</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EBM</th>\n",
       "      <td>6.228575</td>\n",
       "      <td>4.802202</td>\n",
       "      <td>1.880723</td>\n",
       "      <td>1.693768</td>\n",
       "      <td>21.923022</td>\n",
       "      <td>1.168456</td>\n",
       "      <td>1.981410</td>\n",
       "      <td>10.150442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IGANN</th>\n",
       "      <td>2.269603</td>\n",
       "      <td>10.725619</td>\n",
       "      <td>0.986557</td>\n",
       "      <td>0.765108</td>\n",
       "      <td>1.887702</td>\n",
       "      <td>0.433337</td>\n",
       "      <td>0.908650</td>\n",
       "      <td>0.180249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IGANN-IT</th>\n",
       "      <td>4.911774</td>\n",
       "      <td>17.286643</td>\n",
       "      <td>2.491910</td>\n",
       "      <td>1.053885</td>\n",
       "      <td>8.639091</td>\n",
       "      <td>2.845880</td>\n",
       "      <td>1.383201</td>\n",
       "      <td>0.681804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.013115</td>\n",
       "      <td>0.029584</td>\n",
       "      <td>0.007466</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.039186</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>0.003276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>1.805201</td>\n",
       "      <td>2.475679</td>\n",
       "      <td>1.110600</td>\n",
       "      <td>0.257446</td>\n",
       "      <td>7.887241</td>\n",
       "      <td>0.285077</td>\n",
       "      <td>0.329367</td>\n",
       "      <td>0.290999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.004609</td>\n",
       "      <td>0.004884</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.013417</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.002550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset           mean       bike      crab  diabetes    diamond  insurance  \\\n",
       "model                                                                         \n",
       "DecisionTree  0.013839   0.017874  0.008155  0.002927   0.058551   0.002749   \n",
       "EBM           6.228575   4.802202  1.880723  1.693768  21.923022   1.168456   \n",
       "IGANN         2.269603  10.725619  0.986557  0.765108   1.887702   0.433337   \n",
       "IGANN-IT      4.911774  17.286643  2.491910  1.053885   8.639091   2.845880   \n",
       "Lasso         0.013115   0.029584  0.007466  0.003180   0.039186   0.003799   \n",
       "RandomForest  1.805201   2.475679  1.110600  0.257446   7.887241   0.285077   \n",
       "Ridge         0.004609   0.004884  0.003008  0.002117   0.013417   0.002962   \n",
       "\n",
       "dataset       productivity    student  \n",
       "model                                  \n",
       "DecisionTree      0.003563   0.003051  \n",
       "EBM               1.981410  10.150442  \n",
       "IGANN             0.908650   0.180249  \n",
       "IGANN-IT          1.383201   0.681804  \n",
       "Lasso             0.005316   0.003276  \n",
       "RandomForest      0.329367   0.290999  \n",
       "Ridge             0.003322   0.002550  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_time_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa4405",
   "metadata": {},
   "source": [
    "# 5. Evaluation Regression Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570a48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: stroke, Shape: (5110, 12)\n",
      "stroke\n",
      "0    4861\n",
      "1     249\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset: churn, Shape: (7043, 21)\n",
      "Churn\n",
      "No     5174\n",
      "Yes    1869\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset: fico, Shape: (10459, 24)\n",
      "RiskPerformance\n",
      "Bad     5459\n",
      "Good    5000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset: bank, Shape: (45211, 17)\n",
      "y\n",
      "no     39922\n",
      "yes     5289\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset: adult, Shape: (32561, 15)\n",
      "income\n",
      "<=50K    24720\n",
      ">50K      7841\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset: airline, Shape: (103904, 25)\n",
      "satisfaction\n",
      "neutral or dissatisfied    58879\n",
      "satisfied                  45025\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset: college, Shape: (1000, 11)\n",
      "will_go_to_college\n",
      "True     500\n",
      "False    500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset: weather, Shape: (145460, 23)\n",
      "RainTomorrow\n",
      "No     110316\n",
      "Yes     31877\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset: compas, Shape: (7214, 53)\n",
      "two_year_recid\n",
      "0    3963\n",
      "1    3251\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "datasets_filtered = df_overview[(df_overview[\"task_type\"] == \"binary\") & (df_overview[\"name\"] != \"adult\") & (df_overview[\"name\"] != \"airline\") & (df_overview[\"name\"] != \"weather\") & (df_overview[\"name\"] != \"compas\")]\n",
    "\n",
    "for idx, row in datasets_filtered.iterrows():\n",
    "    file_path = f\"{data_path}/{row['name']}/{row['file_name']}\"\n",
    "    data = pd.read_csv(file_path, sep=row[\"separator\"])\n",
    "\n",
    "    print(f\"\\nDataset: {row['name']}, Shape: {data.shape}\")\n",
    "    print(data[row[\"target\"]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "60868240",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_classification = pd.DataFrame(columns=[\"model\", \"dataset\", \"log_loss\", \"pr_auc\", \"f1\", \"train_duration\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5709d337",
   "metadata": {},
   "source": [
    "## 5.1 Logistic Regression (L1 and L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7845115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression_L1 on stroke dataset...\n",
      "Training LogisticRegression_L2 on stroke dataset...\n",
      "Training LogisticRegression_L1 on churn dataset...\n",
      "Training LogisticRegression_L2 on churn dataset...\n",
      "Training LogisticRegression_L1 on fico dataset...\n",
      "Training LogisticRegression_L2 on fico dataset...\n",
      "Training LogisticRegression_L1 on bank dataset...\n",
      "Training LogisticRegression_L2 on bank dataset...\n",
      "Training LogisticRegression_L1 on college dataset...\n",
      "Training LogisticRegression_L2 on college dataset...\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with L1 and L2 regularization (one-hot encoding needed)\n",
    "datasets_filtered = df_overview[(df_overview[\"task_type\"] == \"binary\") & (df_overview[\"name\"] != \"adult\") & (df_overview[\"name\"] != \"airline\") & (df_overview[\"name\"] != \"weather\") & (df_overview[\"name\"] != \"compas\")]\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression_L1\": LogisticRegression(penalty='l1', solver='liblinear', C=1.0, max_iter=1000, random_state=seed),\n",
    "    \"LogisticRegression_L2\": LogisticRegression(penalty='l2', solver='liblinear', C=1.0, max_iter=1000, random_state=seed),\n",
    "}\n",
    "\n",
    "for idx, row in datasets_filtered.iterrows():\n",
    "    # load data\n",
    "    file_name = row[\"file_name\"]\n",
    "    separator = row[\"separator\"]\n",
    "    folder_name = row[\"name\"]\n",
    "    target = row[\"target\"]\n",
    "    file_path = f\"{data_path}/{folder_name}/{file_name}\"\n",
    "\n",
    "    data = pd.read_csv(file_path, sep=separator)\n",
    "    \n",
    "    X = data.drop(columns=target)\n",
    "    y = data[target]\n",
    "\n",
    "    # Preprocessing\n",
    "    # 1. Train-Test-Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
    "\n",
    "    # 2. Split features into numerical and categorical\n",
    "    X_train_num = X_train[row[\"num_features\"]]\n",
    "    X_train_cat = X_train[row[\"cat_features\"]]\n",
    "    X_test_num = X_test[row[\"num_features\"]]\n",
    "    X_test_cat = X_test[row[\"cat_features\"]]\n",
    "\n",
    "    # 3. Replace empty strings and other chars in numerical features with np.nan\n",
    "    X_train_num = X_train_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "    X_test_num = X_test_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "\n",
    "    # 4. Correct data types\n",
    "    X_train_num = X_train_num.astype(float)\n",
    "    X_test_num = X_test_num.astype(float)\n",
    "    X_train_cat = X_train_cat.astype(str)\n",
    "    X_test_cat = X_test_cat.astype(str)\n",
    "\n",
    "    # 5. Impute missing values\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "    X_train_num = imputer_num.fit_transform(X_train_num)\n",
    "    X_train_cat = imputer_cat.fit_transform(X_train_cat)\n",
    "    X_test_num = imputer_num.transform(X_test_num)\n",
    "    X_test_cat = imputer_cat.transform(X_test_cat)\n",
    "\n",
    "    # 6. Scale numerical features and target and one-hot encode categorical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train_num)\n",
    "    X_test_num = scaler.transform(X_test_num)\n",
    "    X_train_cat = pd.DataFrame(X_train_cat, columns=row[\"cat_features\"])\n",
    "    X_test_cat = pd.DataFrame(X_test_cat, columns=row[\"cat_features\"])\n",
    "    X_train_cat = pd.get_dummies(X_train_cat, drop_first=True)\n",
    "    X_test_cat = pd.get_dummies(X_test_cat, drop_first=True)\n",
    "    X_test_cat = X_test_cat.reindex(columns=X_train_cat.columns, fill_value=0)\n",
    "\n",
    "    # 7. Concatenate numerical and categorical features\n",
    "    X_train_num = pd.DataFrame(X_train_num, columns=row[\"num_features\"])\n",
    "    X_test_num = pd.DataFrame(X_test_num, columns=row[\"num_features\"])\n",
    "\n",
    "    X_train = pd.concat([X_train_num, X_train_cat], axis=1)\n",
    "    X_test = pd.concat([X_test_num, X_test_cat], axis=1)\n",
    "\n",
    "    # 9. y_train and y_test to 0 and 1\n",
    "    minority_class = y_train.value_counts().idxmin()\n",
    "    y_train = (y_train == minority_class).astype(int)\n",
    "    y_test = (y_test == minority_class).astype(int)\n",
    "\n",
    "    # 9. Train Models\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name} on {folder_name} dataset...\")\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_duration = time.time() - start_time\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # 10. Evaluate Models\n",
    "        logloss = log_loss(y_test, y_pred_proba)\n",
    "        pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        results_classification.loc[len(results_classification)] = {\n",
    "            'model': name,\n",
    "            'dataset': folder_name,\n",
    "            'log_loss': logloss,\n",
    "            'pr_auc': pr_auc,\n",
    "            'f1': f1,\n",
    "            'train_duration': train_duration}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd57b99",
   "metadata": {},
   "source": [
    "## 5.2 Decision Tree and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d17501d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DecisionTree on stroke dataset...\n",
      "Training RandomForest on stroke dataset...\n",
      "Training DecisionTree on churn dataset...\n",
      "Training RandomForest on churn dataset...\n",
      "Training DecisionTree on fico dataset...\n",
      "Training RandomForest on fico dataset...\n",
      "Training DecisionTree on bank dataset...\n",
      "Training RandomForest on bank dataset...\n",
      "Training DecisionTree on college dataset...\n",
      "Training RandomForest on college dataset...\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree and Random Forest (one-hot encoding needed)\n",
    "datasets_filtered = df_overview[(df_overview[\"task_type\"] == \"binary\") & (df_overview[\"name\"] != \"adult\") & (df_overview[\"name\"] != \"airline\") & (df_overview[\"name\"] != \"weather\") & (df_overview[\"name\"] != \"compas\")]\n",
    "\n",
    "models = {\n",
    "    \"DecisionTree\": DecisionTreeClassifier(max_depth=5, random_state=seed),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, max_depth=10, random_state=seed),\n",
    "}\n",
    "\n",
    "for idx, row in datasets_filtered.iterrows():\n",
    "    # load data\n",
    "    file_name = row[\"file_name\"]\n",
    "    separator = row[\"separator\"]\n",
    "    folder_name = row[\"name\"]\n",
    "    target = row[\"target\"]\n",
    "    file_path = f\"{data_path}/{folder_name}/{file_name}\"\n",
    "\n",
    "    data = pd.read_csv(file_path, sep=separator)\n",
    "    \n",
    "    X = data.drop(columns=target)\n",
    "    y = data[target]\n",
    "\n",
    "    # Preprocessing\n",
    "    # 1. Train-Test-Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
    "\n",
    "    # 2. Split features into numerical and categorical\n",
    "    X_train_num = X_train[row[\"num_features\"]]\n",
    "    X_train_cat = X_train[row[\"cat_features\"]]\n",
    "    X_test_num = X_test[row[\"num_features\"]]\n",
    "    X_test_cat = X_test[row[\"cat_features\"]]\n",
    "\n",
    "    # 3. Replace empty strings and other chars in numerical features with np.nan\n",
    "    X_train_num = X_train_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "    X_test_num = X_test_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "\n",
    "    # 4. Correct data types\n",
    "    X_train_num = X_train_num.astype(float)\n",
    "    X_test_num = X_test_num.astype(float)\n",
    "    X_train_cat = X_train_cat.astype(str)\n",
    "    X_test_cat = X_test_cat.astype(str)\n",
    "\n",
    "    # 5. Impute missing values\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "    X_train_num = imputer_num.fit_transform(X_train_num)\n",
    "    X_train_cat = imputer_cat.fit_transform(X_train_cat)\n",
    "    X_test_num = imputer_num.transform(X_test_num)\n",
    "    X_test_cat = imputer_cat.transform(X_test_cat)\n",
    "\n",
    "    # 6. Scale numerical features and target and one-hot encode categorical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train_num)\n",
    "    X_test_num = scaler.transform(X_test_num)\n",
    "    X_train_cat = pd.DataFrame(X_train_cat, columns=row[\"cat_features\"])\n",
    "    X_test_cat = pd.DataFrame(X_test_cat, columns=row[\"cat_features\"])\n",
    "    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    X_train_cat = encoder.fit_transform(X_train_cat)\n",
    "    X_test_cat = encoder.transform(X_test_cat)\n",
    "\n",
    "    # 7. Concatenate numerical and categorical features\n",
    "    X_train_num = pd.DataFrame(X_train_num, columns=row[\"num_features\"])\n",
    "    X_test_num = pd.DataFrame(X_test_num, columns=row[\"num_features\"])\n",
    "    X_train_cat = pd.DataFrame(X_train_cat, columns=row[\"cat_features\"])\n",
    "    X_test_cat = pd.DataFrame(X_test_cat, columns=row[\"cat_features\"])\n",
    "\n",
    "    X_train = pd.concat([X_train_num, X_train_cat], axis=1)\n",
    "    X_test = pd.concat([X_test_num, X_test_cat], axis=1)\n",
    "\n",
    "    # 9. y_train and y_test to 0 and 1\n",
    "    minority_class = y_train.value_counts().idxmin()\n",
    "    y_train = (y_train == minority_class).astype(int)\n",
    "    y_test = (y_test == minority_class).astype(int)\n",
    "\n",
    "    # 9. Train Models\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name} on {folder_name} dataset...\")\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_duration = time.time() - start_time\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # 10. Evaluate Models\n",
    "        logloss = log_loss(y_test, y_pred_proba)\n",
    "        pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        results_classification.loc[len(results_classification)] = {\n",
    "            'model': name,\n",
    "            'dataset': folder_name,\n",
    "            'log_loss': logloss,\n",
    "            'pr_auc': pr_auc,\n",
    "            'f1': f1,\n",
    "            'train_duration': train_duration}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea8f6c9",
   "metadata": {},
   "source": [
    "## 5.3  EBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2409e5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EBM on stroke dataset...\n",
      "Training EBM on churn dataset...\n",
      "Training EBM on fico dataset...\n",
      "Training EBM on bank dataset...\n",
      "Training EBM on college dataset...\n"
     ]
    }
   ],
   "source": [
    "# EBM\n",
    "datasets_filtered = df_overview[(df_overview[\"task_type\"] == \"binary\") & (df_overview[\"name\"] != \"adult\") & (df_overview[\"name\"] != \"airline\") & (df_overview[\"name\"] != \"weather\") & (df_overview[\"name\"] != \"compas\")]\n",
    "\n",
    "for idx, row in datasets_filtered.iterrows():\n",
    "    # load data\n",
    "    file_name = row[\"file_name\"]\n",
    "    separator = row[\"separator\"]\n",
    "    folder_name = row[\"name\"]\n",
    "    target = row[\"target\"]\n",
    "    file_path = f\"{data_path}/{folder_name}/{file_name}\"\n",
    "\n",
    "    data = pd.read_csv(file_path, sep=separator)\n",
    "    \n",
    "    X = data.drop(columns=target)\n",
    "    y = data[target]\n",
    "\n",
    "    # Preprocessing\n",
    "    # 1. Train-Test-Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
    "\n",
    "    # 2. Split features into numerical and categorical\n",
    "    X_train_num = X_train[row[\"num_features\"]]\n",
    "    X_train_cat = X_train[row[\"cat_features\"]]\n",
    "    X_test_num = X_test[row[\"num_features\"]]\n",
    "    X_test_cat = X_test[row[\"cat_features\"]]\n",
    "\n",
    "    # 3. Replace empty strings and other chars in numerical features with np.nan\n",
    "    X_train_num = X_train_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "    X_test_num = X_test_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "\n",
    "    # 4. Correct data types\n",
    "    X_train_num = X_train_num.astype(float)\n",
    "    X_test_num = X_test_num.astype(float)\n",
    "    X_train_cat = X_train_cat.astype(str)\n",
    "    X_test_cat = X_test_cat.astype(str)\n",
    "\n",
    "    # 5. Impute missing values\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "    X_train_num = imputer_num.fit_transform(X_train_num)\n",
    "    X_train_cat = imputer_cat.fit_transform(X_train_cat)\n",
    "    X_test_num = imputer_num.transform(X_test_num)\n",
    "    X_test_cat = imputer_cat.transform(X_test_cat)\n",
    "\n",
    "    # 6. Scale numerical features and target and one-hot encode categorical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train_num)\n",
    "    X_test_num = scaler.transform(X_test_num)\n",
    "\n",
    "    # 7. Concatenate numerical and categorical features\n",
    "    X_train_num = pd.DataFrame(X_train_num, columns=row[\"num_features\"])\n",
    "    X_test_num = pd.DataFrame(X_test_num, columns=row[\"num_features\"])\n",
    "    X_train_cat = pd.DataFrame(X_train_cat, columns=row[\"cat_features\"])\n",
    "    X_test_cat = pd.DataFrame(X_test_cat, columns=row[\"cat_features\"])\n",
    "\n",
    "    X_train = pd.concat([X_train_num, X_train_cat], axis=1)\n",
    "    X_test = pd.concat([X_test_num, X_test_cat], axis=1)\n",
    "\n",
    "    # 9. y_train and y_test to 0 and 1\n",
    "    minority_class = y_train.value_counts().idxmin()\n",
    "    y_train = (y_train == minority_class).astype(int)\n",
    "    y_test = (y_test == minority_class).astype(int)\n",
    "\n",
    "    # 9. Train Models\n",
    "    model = ExplainableBoostingClassifier(interactions=1, max_bins=64, learning_rate=0.1, max_leaves=3, min_samples_leaf=2, random_state=seed)\n",
    "    print(f\"Training EBM on {folder_name} dataset...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_duration = time.time() - start_time\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # 10. Evaluate Models\n",
    "    logloss = log_loss(y_test, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    results_classification.loc[len(results_classification)] = {\n",
    "        'model': 'EBM',\n",
    "        'dataset': folder_name,\n",
    "        'log_loss': logloss,\n",
    "        'pr_auc': pr_auc,\n",
    "        'f1': f1,\n",
    "        'train_duration': train_duration}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683b141c",
   "metadata": {},
   "source": [
    "## 5.4 IGANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1ce62632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IGANN on stroke dataset...\n",
      "Training IGANN on churn dataset...\n",
      "Training IGANN on fico dataset...\n",
      "Training IGANN on bank dataset...\n",
      "Training IGANN on college dataset...\n"
     ]
    }
   ],
   "source": [
    "# IGANN\n",
    "datasets_filtered = df_overview[(df_overview[\"task_type\"] == \"binary\") & (df_overview[\"name\"] != \"adult\") & (df_overview[\"name\"] != \"airline\") & (df_overview[\"name\"] != \"weather\") & (df_overview[\"name\"] != \"compas\")]\n",
    "\n",
    "for idx, row in datasets_filtered.iterrows():\n",
    "    # load data\n",
    "    file_name = row[\"file_name\"]\n",
    "    separator = row[\"separator\"]\n",
    "    folder_name = row[\"name\"]\n",
    "    target = row[\"target\"]\n",
    "    file_path = f\"{data_path}/{folder_name}/{file_name}\"\n",
    "\n",
    "    data = pd.read_csv(file_path, sep=separator)\n",
    "    \n",
    "    X = data.drop(columns=target)\n",
    "    y = data[target]\n",
    "\n",
    "    # Preprocessing\n",
    "    # 1. Train-Test-Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
    "\n",
    "    # 2. Split features into numerical and categorical\n",
    "    X_train_num = X_train[row[\"num_features\"]]\n",
    "    X_train_cat = X_train[row[\"cat_features\"]]\n",
    "    X_test_num = X_test[row[\"num_features\"]]\n",
    "    X_test_cat = X_test[row[\"cat_features\"]]\n",
    "\n",
    "    # 3. Replace empty strings and other chars in numerical features with np.nan\n",
    "    X_train_num = X_train_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "    X_test_num = X_test_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "\n",
    "    # 4. Correct data types\n",
    "    X_train_num = X_train_num.astype(float)\n",
    "    X_test_num = X_test_num.astype(float)\n",
    "    X_train_cat = X_train_cat.astype(str)\n",
    "    X_test_cat = X_test_cat.astype(str)\n",
    "\n",
    "    # 5. Impute missing values\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "    X_train_num = imputer_num.fit_transform(X_train_num)\n",
    "    X_train_cat = imputer_cat.fit_transform(X_train_cat)\n",
    "    X_test_num = imputer_num.transform(X_test_num)\n",
    "    X_test_cat = imputer_cat.transform(X_test_cat)\n",
    "\n",
    "    # 6. Scale numerical features and target and one-hot encode categorical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train_num)\n",
    "    X_test_num = scaler.transform(X_test_num)\n",
    "\n",
    "    # 7. Concatenate numerical and categorical features\n",
    "    X_train_num = pd.DataFrame(X_train_num, columns=row[\"num_features\"])\n",
    "    X_test_num = pd.DataFrame(X_test_num, columns=row[\"num_features\"])\n",
    "    X_train_cat = pd.DataFrame(X_train_cat, columns=row[\"cat_features\"])\n",
    "    X_test_cat = pd.DataFrame(X_test_cat, columns=row[\"cat_features\"])\n",
    "\n",
    "    X_train = pd.concat([X_train_num, X_train_cat], axis=1)\n",
    "    X_test = pd.concat([X_test_num, X_test_cat], axis=1)\n",
    "\n",
    "    # 9. y_train and y_test to 0 and 1\n",
    "    minority_class = y_train.value_counts().idxmin()\n",
    "    y_train = (y_train == minority_class).astype(int)\n",
    "    y_test = (y_test == minority_class).astype(int)\n",
    "\n",
    "    # 9. Train Models\n",
    "    model = IGANN(task=\"classification\", n_hid=10, igann_it=False)\n",
    "    print(f\"Training IGANN on {folder_name} dataset...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_duration = time.time() - start_time\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # 10. Evaluate Models\n",
    "    logloss = log_loss(y_test, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    results_classification.loc[len(results_classification)] = {\n",
    "        'model': 'IGANN',\n",
    "        'dataset': folder_name,\n",
    "        'log_loss': logloss,\n",
    "        'pr_auc': pr_auc,\n",
    "        'f1': f1,\n",
    "        'train_duration': train_duration}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beca153",
   "metadata": {},
   "source": [
    "## 5.5 IGANN-IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e0fccd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IGANN on stroke dataset...\n",
      "Training IGANN on churn dataset...\n",
      "Training IGANN on fico dataset...\n",
      "Training IGANN on bank dataset...\n",
      "Training IGANN on college dataset...\n",
      "No feature combination found. Model does not capture interactions. Try different feature interaction detection method.\n"
     ]
    }
   ],
   "source": [
    "# IGANN-IT\n",
    "datasets_filtered = df_overview[(df_overview[\"task_type\"] == \"binary\") & (df_overview[\"name\"] != \"adult\") & (df_overview[\"name\"] != \"airline\") & (df_overview[\"name\"] != \"weather\") & (df_overview[\"name\"] != \"compas\")]\n",
    "\n",
    "for idx, row in datasets_filtered.iterrows():\n",
    "    # load data\n",
    "    file_name = row[\"file_name\"]\n",
    "    separator = row[\"separator\"]\n",
    "    folder_name = row[\"name\"]\n",
    "    target = row[\"target\"]\n",
    "    file_path = f\"{data_path}/{folder_name}/{file_name}\"\n",
    "\n",
    "    data = pd.read_csv(file_path, sep=separator)\n",
    "    \n",
    "    X = data.drop(columns=target)\n",
    "    y = data[target]\n",
    "\n",
    "    # Preprocessing\n",
    "    # 1. Train-Test-Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
    "\n",
    "    # 2. Split features into numerical and categorical\n",
    "    X_train_num = X_train[row[\"num_features\"]]\n",
    "    X_train_cat = X_train[row[\"cat_features\"]]\n",
    "    X_test_num = X_test[row[\"num_features\"]]\n",
    "    X_test_cat = X_test[row[\"cat_features\"]]\n",
    "\n",
    "    # 3. Replace empty strings and other chars in numerical features with np.nan\n",
    "    X_train_num = X_train_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "    X_test_num = X_test_num.replace({\"\": np.nan, \" \": np.nan, \"?\": np.nan})\n",
    "\n",
    "    # 4. Correct data types\n",
    "    X_train_num = X_train_num.astype(float)\n",
    "    X_test_num = X_test_num.astype(float)\n",
    "    X_train_cat = X_train_cat.astype(str)\n",
    "    X_test_cat = X_test_cat.astype(str)\n",
    "\n",
    "    # 5. Impute missing values\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "    X_train_num = imputer_num.fit_transform(X_train_num)\n",
    "    X_train_cat = imputer_cat.fit_transform(X_train_cat)\n",
    "    X_test_num = imputer_num.transform(X_test_num)\n",
    "    X_test_cat = imputer_cat.transform(X_test_cat)\n",
    "\n",
    "    # 6. Scale numerical features and target and one-hot encode categorical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train_num)\n",
    "    X_test_num = scaler.transform(X_test_num)\n",
    "\n",
    "    # 7. Concatenate numerical and categorical features\n",
    "    X_train_num = pd.DataFrame(X_train_num, columns=row[\"num_features\"])\n",
    "    X_test_num = pd.DataFrame(X_test_num, columns=row[\"num_features\"])\n",
    "    X_train_cat = pd.DataFrame(X_train_cat, columns=row[\"cat_features\"])\n",
    "    X_test_cat = pd.DataFrame(X_test_cat, columns=row[\"cat_features\"])\n",
    "\n",
    "    X_train = pd.concat([X_train_num, X_train_cat], axis=1)\n",
    "    X_test = pd.concat([X_test_num, X_test_cat], axis=1)\n",
    "\n",
    "    # 9. y_train and y_test to 0 and 1\n",
    "    minority_class = y_train.value_counts().idxmin()\n",
    "    y_train = (y_train == minority_class).astype(int)\n",
    "    y_test = (y_test == minority_class).astype(int)\n",
    "\n",
    "    # 9. Train Models\n",
    "    model = IGANN(task=\"classification\", n_hid=10, igann_it=True, interaction_detection_method=\"rulefit\")\n",
    "    print(f\"Training IGANN on {folder_name} dataset...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_duration = time.time() - start_time\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # 10. Evaluate Models\n",
    "    logloss = log_loss(y_test, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    results_classification.loc[len(results_classification)] = {\n",
    "        'model': 'IGANN-IT',\n",
    "        'dataset': folder_name,\n",
    "        'log_loss': logloss,\n",
    "        'pr_auc': pr_auc,\n",
    "        'f1': f1,\n",
    "        'train_duration': train_duration}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d29224",
   "metadata": {},
   "source": [
    "## 5.6 Results Classification Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6c0bb4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_metric_df(metric):\n",
    "    pivot_df = results_classification.pivot_table(index='model', columns='dataset', values=metric)\n",
    "    pivot_df['mean'] = pivot_df.mean(axis=1)\n",
    "    pivot_df = pivot_df[['mean'] + [col for col in pivot_df.columns if col != 'mean']]\n",
    "    return pivot_df\n",
    "\n",
    "logloss_df = create_classification_metric_df('log_loss')\n",
    "prauc_df = create_classification_metric_df('pr_auc')\n",
    "f1_df = create_classification_metric_df('f1')  # falls du F1 misst\n",
    "train_time_df = create_classification_metric_df('train_duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "277f8324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset</th>\n",
       "      <th>mean</th>\n",
       "      <th>bank</th>\n",
       "      <th>churn</th>\n",
       "      <th>college</th>\n",
       "      <th>fico</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>0.611913</td>\n",
       "      <td>0.341970</td>\n",
       "      <td>0.504942</td>\n",
       "      <td>0.878982</td>\n",
       "      <td>0.647117</td>\n",
       "      <td>0.686555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EBM</th>\n",
       "      <td>0.343272</td>\n",
       "      <td>0.298289</td>\n",
       "      <td>0.399600</td>\n",
       "      <td>0.270529</td>\n",
       "      <td>0.560170</td>\n",
       "      <td>0.187771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IGANN</th>\n",
       "      <td>0.352934</td>\n",
       "      <td>0.302892</td>\n",
       "      <td>0.406086</td>\n",
       "      <td>0.294825</td>\n",
       "      <td>0.572233</td>\n",
       "      <td>0.188633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IGANN-IT</th>\n",
       "      <td>0.352431</td>\n",
       "      <td>0.300378</td>\n",
       "      <td>0.406086</td>\n",
       "      <td>0.294825</td>\n",
       "      <td>0.572233</td>\n",
       "      <td>0.188633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_L1</th>\n",
       "      <td>0.361939</td>\n",
       "      <td>0.305582</td>\n",
       "      <td>0.406098</td>\n",
       "      <td>0.326884</td>\n",
       "      <td>0.582500</td>\n",
       "      <td>0.188633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_L2</th>\n",
       "      <td>0.362872</td>\n",
       "      <td>0.305557</td>\n",
       "      <td>0.406027</td>\n",
       "      <td>0.331676</td>\n",
       "      <td>0.581795</td>\n",
       "      <td>0.189305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.354042</td>\n",
       "      <td>0.300309</td>\n",
       "      <td>0.410178</td>\n",
       "      <td>0.271934</td>\n",
       "      <td>0.565926</td>\n",
       "      <td>0.221863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset                    mean      bank     churn   college      fico  \\\n",
       "model                                                                     \n",
       "DecisionTree           0.611913  0.341970  0.504942  0.878982  0.647117   \n",
       "EBM                    0.343272  0.298289  0.399600  0.270529  0.560170   \n",
       "IGANN                  0.352934  0.302892  0.406086  0.294825  0.572233   \n",
       "IGANN-IT               0.352431  0.300378  0.406086  0.294825  0.572233   \n",
       "LogisticRegression_L1  0.361939  0.305582  0.406098  0.326884  0.582500   \n",
       "LogisticRegression_L2  0.362872  0.305557  0.406027  0.331676  0.581795   \n",
       "RandomForest           0.354042  0.300309  0.410178  0.271934  0.565926   \n",
       "\n",
       "dataset                  stroke  \n",
       "model                            \n",
       "DecisionTree           0.686555  \n",
       "EBM                    0.187771  \n",
       "IGANN                  0.188633  \n",
       "IGANN-IT               0.188633  \n",
       "LogisticRegression_L1  0.188633  \n",
       "LogisticRegression_L2  0.189305  \n",
       "RandomForest           0.221863  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "20d15835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset</th>\n",
       "      <th>mean</th>\n",
       "      <th>bank</th>\n",
       "      <th>churn</th>\n",
       "      <th>college</th>\n",
       "      <th>fico</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>0.552865</td>\n",
       "      <td>0.328163</td>\n",
       "      <td>0.624367</td>\n",
       "      <td>0.914755</td>\n",
       "      <td>0.710950</td>\n",
       "      <td>0.186089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EBM</th>\n",
       "      <td>0.625602</td>\n",
       "      <td>0.433338</td>\n",
       "      <td>0.701364</td>\n",
       "      <td>0.964446</td>\n",
       "      <td>0.760377</td>\n",
       "      <td>0.268485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IGANN</th>\n",
       "      <td>0.617536</td>\n",
       "      <td>0.418484</td>\n",
       "      <td>0.686792</td>\n",
       "      <td>0.955758</td>\n",
       "      <td>0.743360</td>\n",
       "      <td>0.283288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IGANN-IT</th>\n",
       "      <td>0.619451</td>\n",
       "      <td>0.428059</td>\n",
       "      <td>0.686792</td>\n",
       "      <td>0.955758</td>\n",
       "      <td>0.743360</td>\n",
       "      <td>0.283288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_L1</th>\n",
       "      <td>0.611140</td>\n",
       "      <td>0.415437</td>\n",
       "      <td>0.686904</td>\n",
       "      <td>0.939384</td>\n",
       "      <td>0.730686</td>\n",
       "      <td>0.283288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_L2</th>\n",
       "      <td>0.609905</td>\n",
       "      <td>0.415376</td>\n",
       "      <td>0.686721</td>\n",
       "      <td>0.936816</td>\n",
       "      <td>0.731517</td>\n",
       "      <td>0.279093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.616262</td>\n",
       "      <td>0.425920</td>\n",
       "      <td>0.686032</td>\n",
       "      <td>0.969084</td>\n",
       "      <td>0.754586</td>\n",
       "      <td>0.245686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset                    mean      bank     churn   college      fico  \\\n",
       "model                                                                     \n",
       "DecisionTree           0.552865  0.328163  0.624367  0.914755  0.710950   \n",
       "EBM                    0.625602  0.433338  0.701364  0.964446  0.760377   \n",
       "IGANN                  0.617536  0.418484  0.686792  0.955758  0.743360   \n",
       "IGANN-IT               0.619451  0.428059  0.686792  0.955758  0.743360   \n",
       "LogisticRegression_L1  0.611140  0.415437  0.686904  0.939384  0.730686   \n",
       "LogisticRegression_L2  0.609905  0.415376  0.686721  0.936816  0.731517   \n",
       "RandomForest           0.616262  0.425920  0.686032  0.969084  0.754586   \n",
       "\n",
       "dataset                  stroke  \n",
       "model                            \n",
       "DecisionTree           0.186089  \n",
       "EBM                    0.268485  \n",
       "IGANN                  0.283288  \n",
       "IGANN-IT               0.283288  \n",
       "LogisticRegression_L1  0.283288  \n",
       "LogisticRegression_L2  0.279093  \n",
       "RandomForest           0.245686  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prauc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3a9cb86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset</th>\n",
       "      <th>mean</th>\n",
       "      <th>bank</th>\n",
       "      <th>churn</th>\n",
       "      <th>college</th>\n",
       "      <th>fico</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>0.496846</td>\n",
       "      <td>0.303132</td>\n",
       "      <td>0.620619</td>\n",
       "      <td>0.837398</td>\n",
       "      <td>0.659924</td>\n",
       "      <td>0.063158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EBM</th>\n",
       "      <td>0.500544</td>\n",
       "      <td>0.308807</td>\n",
       "      <td>0.598817</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>0.684909</td>\n",
       "      <td>0.024691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IGANN</th>\n",
       "      <td>0.494086</td>\n",
       "      <td>0.291762</td>\n",
       "      <td>0.628959</td>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.679120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IGANN-IT</th>\n",
       "      <td>0.496064</td>\n",
       "      <td>0.301651</td>\n",
       "      <td>0.628959</td>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.679120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_L1</th>\n",
       "      <td>0.489626</td>\n",
       "      <td>0.285052</td>\n",
       "      <td>0.628959</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.671375</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_L2</th>\n",
       "      <td>0.487773</td>\n",
       "      <td>0.283227</td>\n",
       "      <td>0.628249</td>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.672485</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.482935</td>\n",
       "      <td>0.282339</td>\n",
       "      <td>0.563177</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.684541</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset                    mean      bank     churn   college      fico  \\\n",
       "model                                                                     \n",
       "DecisionTree           0.496846  0.303132  0.620619  0.837398  0.659924   \n",
       "EBM                    0.500544  0.308807  0.598817  0.885496  0.684909   \n",
       "IGANN                  0.494086  0.291762  0.628959  0.870588  0.679120   \n",
       "IGANN-IT               0.496064  0.301651  0.628959  0.870588  0.679120   \n",
       "LogisticRegression_L1  0.489626  0.285052  0.628959  0.862745  0.671375   \n",
       "LogisticRegression_L2  0.487773  0.283227  0.628249  0.854902  0.672485   \n",
       "RandomForest           0.482935  0.282339  0.563177  0.884615  0.684541   \n",
       "\n",
       "dataset                  stroke  \n",
       "model                            \n",
       "DecisionTree           0.063158  \n",
       "EBM                    0.024691  \n",
       "IGANN                  0.000000  \n",
       "IGANN-IT               0.000000  \n",
       "LogisticRegression_L1  0.000000  \n",
       "LogisticRegression_L2  0.000000  \n",
       "RandomForest           0.000000  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3a175eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset</th>\n",
       "      <th>mean</th>\n",
       "      <th>bank</th>\n",
       "      <th>churn</th>\n",
       "      <th>college</th>\n",
       "      <th>fico</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>0.021938</td>\n",
       "      <td>0.056432</td>\n",
       "      <td>0.013702</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.027123</td>\n",
       "      <td>0.008215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EBM</th>\n",
       "      <td>3.863355</td>\n",
       "      <td>6.548609</td>\n",
       "      <td>2.565856</td>\n",
       "      <td>1.146001</td>\n",
       "      <td>2.225947</td>\n",
       "      <td>6.830360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IGANN</th>\n",
       "      <td>3.649745</td>\n",
       "      <td>13.807416</td>\n",
       "      <td>0.506658</td>\n",
       "      <td>0.581314</td>\n",
       "      <td>2.596665</td>\n",
       "      <td>0.756672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IGANN-IT</th>\n",
       "      <td>4.160512</td>\n",
       "      <td>11.500682</td>\n",
       "      <td>2.657727</td>\n",
       "      <td>0.560003</td>\n",
       "      <td>3.738900</td>\n",
       "      <td>2.345250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_L1</th>\n",
       "      <td>0.286660</td>\n",
       "      <td>0.510881</td>\n",
       "      <td>0.385161</td>\n",
       "      <td>0.006539</td>\n",
       "      <td>0.510507</td>\n",
       "      <td>0.020210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_L2</th>\n",
       "      <td>0.067794</td>\n",
       "      <td>0.212105</td>\n",
       "      <td>0.019340</td>\n",
       "      <td>0.003388</td>\n",
       "      <td>0.093327</td>\n",
       "      <td>0.010809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.875594</td>\n",
       "      <td>2.282170</td>\n",
       "      <td>0.551217</td>\n",
       "      <td>0.199228</td>\n",
       "      <td>0.967871</td>\n",
       "      <td>0.377486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset                    mean       bank     churn   college      fico  \\\n",
       "model                                                                      \n",
       "DecisionTree           0.021938   0.056432  0.013702  0.004217  0.027123   \n",
       "EBM                    3.863355   6.548609  2.565856  1.146001  2.225947   \n",
       "IGANN                  3.649745  13.807416  0.506658  0.581314  2.596665   \n",
       "IGANN-IT               4.160512  11.500682  2.657727  0.560003  3.738900   \n",
       "LogisticRegression_L1  0.286660   0.510881  0.385161  0.006539  0.510507   \n",
       "LogisticRegression_L2  0.067794   0.212105  0.019340  0.003388  0.093327   \n",
       "RandomForest           0.875594   2.282170  0.551217  0.199228  0.967871   \n",
       "\n",
       "dataset                  stroke  \n",
       "model                            \n",
       "DecisionTree           0.008215  \n",
       "EBM                    6.830360  \n",
       "IGANN                  0.756672  \n",
       "IGANN-IT               2.345250  \n",
       "LogisticRegression_L1  0.020210  \n",
       "LogisticRegression_L2  0.010809  \n",
       "RandomForest           0.377486  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be52d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "\n",
    "# num_samples = 5000\n",
    "\n",
    "# fahrleistung = np.random.randint(5000, 50000, num_samples)  # Annual mileage in km\n",
    "# fahrzeugtyp = np.random.choice([\"Kleinwagen\", \"SUV\", \"Sportwagen\", \"Transporter\"], num_samples)\n",
    "# alter_fahrer = np.random.randint(18, 80, num_samples)  # Age of driver\n",
    "# anzahl_unfälle = np.random.poisson(0.5, num_samples)  # Previous accidents (Poisson distribution)\n",
    "# region = np.random.choice([\"ländlich\", \"städtisch\", \"Metropole\"], num_samples)\n",
    "\n",
    "# # Define probability of an accident based on features\n",
    "# base_prob = 0.02  # Base probability of accident\n",
    "\n",
    "# # Interaction effect: Higher mileage increases risk for Sportwagen & Transporter more\n",
    "# prob_fahrleistung = np.zeros(num_samples)\n",
    "\n",
    "# # Different effects for vehicle types\n",
    "# fahrzeugtyp_effect = np.zeros(num_samples)\n",
    "\n",
    "# interaktion_fahrleistung_typ = np.array([\n",
    "#     (f / 50000) * (\n",
    "#         0.01 if t == \"Kleinwagen\" else\n",
    "#         0.03 if t == \"SUV\" else\n",
    "#         0.10 if t == \"Sportwagen\" else\n",
    "#         0.08  # Transporter\n",
    "#     ) for f, t in zip(fahrleistung, fahrzeugtyp)\n",
    "# ])\n",
    "\n",
    "# # Age effect: Younger and older drivers have higher risk\n",
    "# alter_effect = np.where((alter_fahrer < 25) | (alter_fahrer > 65), 0.05, 0.02)\n",
    "\n",
    "# # More past accidents -> higher probability\n",
    "# unfall_effect = anzahl_unfälle * 0.04\n",
    "\n",
    "# # Regional effect: More accidents in cities & metropolitan areas\n",
    "# region_effect = np.array([\n",
    "#     0.01 if r == \"ländlich\" else\n",
    "#     0.03 if r == \"städtisch\" else\n",
    "#     0.05 for r in region\n",
    "# ])\n",
    "\n",
    "# # Compute final probability (capped at 1)\n",
    "# final_prob = np.clip(\n",
    "#     base_prob + interaktion_fahrleistung_typ + alter_effect + unfall_effect + region_effect,\n",
    "#     0, 1\n",
    "# )\n",
    "# # Generate binary target variable (1 = accident, 0 = no accident) using probabilities\n",
    "# y = np.random.binomial(1, final_prob, num_samples)\n",
    "\n",
    "# # Create DataFrame\n",
    "# X = pd.DataFrame({\n",
    "#     \"Jährliche Fahrleistung (km)\": fahrleistung,\n",
    "#     \"Fahrzeugtyp\": fahrzeugtyp,\n",
    "#     \"Alter Fahrer\": alter_fahrer,\n",
    "#     \"Anzahl früherer Unfälle\": anzahl_unfälle,\n",
    "#     \"Region\": region\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ab423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# num_samples = 10000\n",
    "\n",
    "# # Merkmale generieren\n",
    "# fahrleistung = np.random.randint(5000, 50000, num_samples)  # Annual mileage in km\n",
    "# fahrzeugtyp = np.random.choice([\"Kleinwagen\", \"SUV\", \"Sportwagen\", \"Transporter\"], num_samples)\n",
    "# alter_fahrer = np.random.randint(18, 80, num_samples)  # Age of driver\n",
    "# anzahl_unfälle = np.random.poisson(0.5, num_samples)  # Previous accidents\n",
    "# region = np.random.choice([\"ländlich\", \"städtisch\", \"Metropole\"], num_samples)\n",
    "\n",
    "# base_prob = 0.350  # statt 0.02\n",
    "\n",
    "# interaktion_fahrleistung_typ = np.array([\n",
    "#     (f / 50000) * (\n",
    "#         0.02 if t == \"Kleinwagen\" else\n",
    "#         0.05 if t == \"SUV\" else\n",
    "#         0.25 if t == \"Sportwagen\" else\n",
    "#         0.20  # Transporter\n",
    "#     ) for f, t in zip(fahrleistung, fahrzeugtyp)\n",
    "# ])\n",
    "\n",
    "# # Alterseffekt: höhere Gefahr für Jüngere und Ältere\n",
    "# alter_effect = np.where((alter_fahrer < 25) | (alter_fahrer > 65), 0.05, 0.02)\n",
    "\n",
    "# # Unfallhistorie\n",
    "# unfall_effect = anzahl_unfälle * 0.04\n",
    "\n",
    "# # Regionseffekt\n",
    "# region_effect = np.array([\n",
    "#     0.01 if r == \"ländlich\" else\n",
    "#     0.03 if r == \"städtisch\" else\n",
    "#     0.05 for r in region\n",
    "# ])\n",
    "\n",
    "# # Finale Unfallwahrscheinlichkeit\n",
    "# final_prob = np.clip(\n",
    "#     base_prob + interaktion_fahrleistung_typ + alter_effect + unfall_effect + region_effect,\n",
    "#     0, 1\n",
    "# )\n",
    "\n",
    "# # Zielvariable (Unfall: 1 = ja, 0 = nein)\n",
    "# y = np.random.binomial(1, final_prob, num_samples)\n",
    "\n",
    "# # DataFrame\n",
    "# X = pd.DataFrame({\n",
    "#     \"Jährliche Fahrleistung (km)\": fahrleistung,\n",
    "#     \"Fahrzeugtyp\": fahrzeugtyp,\n",
    "#     \"Alter Fahrer\": alter_fahrer,\n",
    "#     \"Anzahl früherer Unfälle\": anzahl_unfälle,\n",
    "#     \"Region\": region\n",
    "# })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c830a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scale data\n",
    "# scaler = StandardScaler()\n",
    "# continuous_features = sorted(['Jährliche Fahrleistung (km)', 'Alter Fahrer', 'Anzahl früherer Unfälle'])\n",
    "# X_num = scaler.fit_transform(X[continuous_features])\n",
    "# X_cat = X.drop(columns=continuous_features)\n",
    "# X = pd.concat([pd.DataFrame(X_num, columns=continuous_features), X_cat], axis=1)\n",
    "\n",
    "# # train-test-split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29215a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n",
      "F1 Score: 0.55\n",
      "ROC AUC: 0.55\n"
     ]
    }
   ],
   "source": [
    "# model = IGANN(task='classification', n_hid=10, igann_it=False)\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "# roc_auc = roc_auc_score(y_test, y_pred)\n",
    "# print(f\"Accuracy: {accuracy:.2f}\")\n",
    "# print(f\"F1 Score: {f1:.2f}\")\n",
    "# print(f\"ROC AUC: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49da43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n",
      "F1 Score: 0.55\n",
      "ROC AUC: 0.56\n"
     ]
    }
   ],
   "source": [
    "# model = IGANN(task='classification', n_hid=10, igann_it=True, interaction_detection_method=\"rulefit\")\n",
    "# model.fit(X, y)\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "# roc_auc = roc_auc_score(y_test, y_pred)\n",
    "# print(f\"Accuracy: {accuracy:.2f}\")\n",
    "# print(f\"F1 Score: {f1:.2f}\")\n",
    "# print(f\"ROC AUC: {roc_auc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
